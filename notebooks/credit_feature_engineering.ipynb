{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38870de9",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Preparaci√≥n de Caracter√≠sticas para Modelado Crediticio\n",
    "\n",
    "Este documento es la continuaci√≥n de mi an√°lisis exploratorio de datos (EDA) realizado en `eda.ipynb`, donde guard√© el dataset resultante en `../data/processed/eda_processed/eda_result.csv`. Aqu√≠ transformar√© y optimizar√© ese dataset para un modelo de Machine Learning, definiendo mis decisiones finales sobre qu√© columnas eliminar, mantener y c√≥mo transformarlas, adem√°s de crear nuevas caracter√≠sticas que enriquezcan mi an√°lisis. Mi foco est√° en lograr un dataset limpio, representativo y listo para predecir el comportamiento crediticio. A continuaci√≥n, detallo los pasos clave que guiar√© en este archivo:\n",
    "\n",
    "1. **‚úÖ Mis Decisiones Iniciales**: Eliminar√© columnas irrelevantes o redundantes (como `ID`, `Customer_ID`, `Mes`, `Ingreso_Anual`, `Num_Prestamos`), mantendr√© las m√°s predictivas (como `Edad_Historial_Credito`, `Deuda_Pendiente`, `Tasa_Interes`) y planificar√© transformaciones iniciales.  \n",
    "2. **üîç Mi Revisi√≥n de Edad y Historial Crediticio**: Ajustar√© incoherencias en `Edad_Historial_Credito` con un criterio din√°mico (ej. historial m√°ximo desde los 18 a√±os), superando el umbral arbitrario de 1.5 que us√© previamente.  \n",
    "3. **üìä Mi Evaluaci√≥n de Eliminaci√≥n de Datos**: Reconsiderar√© la eliminaci√≥n de clientes con 0 cuentas o pr√©stamos (14% del dataset), evaluando su impacto en la representatividad.  \n",
    "4. **‚ú® Mi Creaci√≥n de Nuevas Caracter√≠sticas**: Generar√© variables como `debt_to_income` (`Deuda_Pendiente` / `Salario_Mensual`), `payment_to_income` (`Total_Cuota_Mensual` / `Salario_Mensual`), `credit_history_ratio` (`Edad_Historial_Credito` / `Edad`), `delay_ratio` (`Retraso_Pago` / `Num_Pagos_Retrasados`) y `credit_usage_to_limit` (`Deuda_Pendiente` / `Cambio_Limite_Credito`) para capturar relaciones financieras clave.  \n",
    "5. **üìè Mis Transformaciones Num√©ricas**: Escalar√© columnas como `Deuda_Pendiente` y `Tasa_Interes` con `StandardScaler` y aplicar√© capping a outliers (ej. percentil 99) para estabilizar rangos dispares.  \n",
    "6. **üîß Mi Codificaci√≥n Avanzada**: Simplificar√© `Comportamiento_Pago` en categor√≠as m√°s manejables antes de aplicar `OneHotEncoder`, y probar√© `LabelEncoder` en `Mezcla_Crediticia` para aprovechar su orden natural, compar√°ndolo con otras opciones.  \n",
    "7. **üèÜ Mi Validaci√≥n**: Reentrenar√© un `RandomForestClassifier` con las nuevas caracter√≠sticas para medir su importancia y confirmar que mi dataset est√° optimizado.  \n",
    "\n",
    "Con este enfoque, busco un balance entre limpieza, enriquecimiento y utilidad predictiva, preparando el terreno para un modelado efectivo. ¬°Manos a la obra! üöÄ\n",
    "\n",
    "## 1. üìã Mi Carga del Dataset\n",
    "Cargo el dataset procesado del EDA, guardado en `../data/processed/eda_processed/eda_result.csv`, como punto de partida para mi ingenier√≠a de caracter√≠sticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1dd9147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ID_Cliente</th>\n",
       "      <th>Mes</th>\n",
       "      <th>Nombre</th>\n",
       "      <th>Edad</th>\n",
       "      <th>Numero_Seguro_Social</th>\n",
       "      <th>Ocupacion</th>\n",
       "      <th>Ingreso_Anual</th>\n",
       "      <th>Salario_Mensual</th>\n",
       "      <th>Num_Cuentas_Bancarias</th>\n",
       "      <th>Num_Tarjetas_Credito</th>\n",
       "      <th>Tasa_Interes</th>\n",
       "      <th>Num_Prestamos</th>\n",
       "      <th>Tipo_Prestamo</th>\n",
       "      <th>Retraso_Pago</th>\n",
       "      <th>Num_Pagos_Retrasados</th>\n",
       "      <th>Cambio_Limite_Credito</th>\n",
       "      <th>Num_Consultas_Credito</th>\n",
       "      <th>Mezcla_Crediticia</th>\n",
       "      <th>Deuda_Pendiente</th>\n",
       "      <th>Ratio_Utilizacion_Credito</th>\n",
       "      <th>Edad_Historial_Credito</th>\n",
       "      <th>Pago_Minimo</th>\n",
       "      <th>Total_Cuota_Mensual</th>\n",
       "      <th>Inversion_Mensual</th>\n",
       "      <th>Comportamiento_Pago</th>\n",
       "      <th>Saldo_Mensual</th>\n",
       "      <th>Puntaje_Credito</th>\n",
       "      <th>Puntaje_Credito_Num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5634</td>\n",
       "      <td>3392</td>\n",
       "      <td>1</td>\n",
       "      <td>Aaron Maashoh</td>\n",
       "      <td>23.0</td>\n",
       "      <td>821000265.0</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>19114.12</td>\n",
       "      <td>1824.843333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>809.98</td>\n",
       "      <td>26.822620</td>\n",
       "      <td>265.0</td>\n",
       "      <td>No</td>\n",
       "      <td>49.574949</td>\n",
       "      <td>21.46538</td>\n",
       "      <td>High_spent_Small_value_payments</td>\n",
       "      <td>312.494089</td>\n",
       "      <td>Good</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5635</td>\n",
       "      <td>3392</td>\n",
       "      <td>2</td>\n",
       "      <td>Aaron Maashoh</td>\n",
       "      <td>23.0</td>\n",
       "      <td>821000265.0</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>19114.12</td>\n",
       "      <td>1824.843333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>809.98</td>\n",
       "      <td>31.944960</td>\n",
       "      <td>266.0</td>\n",
       "      <td>No</td>\n",
       "      <td>49.574949</td>\n",
       "      <td>21.46538</td>\n",
       "      <td>Low_spent_Large_value_payments</td>\n",
       "      <td>284.629162</td>\n",
       "      <td>Good</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5636</td>\n",
       "      <td>3392</td>\n",
       "      <td>3</td>\n",
       "      <td>Aaron Maashoh</td>\n",
       "      <td>23.0</td>\n",
       "      <td>821000265.0</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>19114.12</td>\n",
       "      <td>1824.843333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>809.98</td>\n",
       "      <td>28.609352</td>\n",
       "      <td>267.0</td>\n",
       "      <td>No</td>\n",
       "      <td>49.574949</td>\n",
       "      <td>21.46538</td>\n",
       "      <td>Low_spent_Medium_value_payments</td>\n",
       "      <td>331.209863</td>\n",
       "      <td>Good</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5637</td>\n",
       "      <td>3392</td>\n",
       "      <td>4</td>\n",
       "      <td>Aaron Maashoh</td>\n",
       "      <td>23.0</td>\n",
       "      <td>821000265.0</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>19114.12</td>\n",
       "      <td>1824.843333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>809.98</td>\n",
       "      <td>31.377862</td>\n",
       "      <td>268.0</td>\n",
       "      <td>No</td>\n",
       "      <td>49.574949</td>\n",
       "      <td>21.46538</td>\n",
       "      <td>Low_spent_Small_value_payments</td>\n",
       "      <td>223.451310</td>\n",
       "      <td>Good</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5638</td>\n",
       "      <td>3392</td>\n",
       "      <td>5</td>\n",
       "      <td>Aaron Maashoh</td>\n",
       "      <td>23.0</td>\n",
       "      <td>821000265.0</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>19114.12</td>\n",
       "      <td>1824.843333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>809.98</td>\n",
       "      <td>24.797347</td>\n",
       "      <td>269.0</td>\n",
       "      <td>No</td>\n",
       "      <td>49.574949</td>\n",
       "      <td>21.46538</td>\n",
       "      <td>High_spent_Medium_value_payments</td>\n",
       "      <td>341.489231</td>\n",
       "      <td>Good</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  ID_Cliente  Mes         Nombre  Edad  Numero_Seguro_Social  Ocupacion  Ingreso_Anual  Salario_Mensual  Num_Cuentas_Bancarias  Num_Tarjetas_Credito  Tasa_Interes  Num_Prestamos                                                        Tipo_Prestamo  Retraso_Pago  Num_Pagos_Retrasados  Cambio_Limite_Credito  Num_Consultas_Credito Mezcla_Crediticia  Deuda_Pendiente  Ratio_Utilizacion_Credito  Edad_Historial_Credito Pago_Minimo  Total_Cuota_Mensual  Inversion_Mensual               Comportamiento_Pago  Saldo_Mensual Puntaje_Credito  Puntaje_Credito_Num\n",
       "0  5634        3392    1  Aaron Maashoh  23.0           821000265.0  Scientist       19114.12      1824.843333                    3.0                   4.0           3.0            4.0  Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan           3.0                   7.0                  11.27                    4.0              Good           809.98                  26.822620                   265.0          No            49.574949           21.46538   High_spent_Small_value_payments     312.494089            Good                    0\n",
       "1  5635        3392    2  Aaron Maashoh  23.0           821000265.0  Scientist       19114.12      1824.843333                    3.0                   4.0           3.0            4.0  Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan           3.0                   4.0                  11.27                    4.0              Good           809.98                  31.944960                   266.0          No            49.574949           21.46538    Low_spent_Large_value_payments     284.629162            Good                    0\n",
       "2  5636        3392    3  Aaron Maashoh  23.0           821000265.0  Scientist       19114.12      1824.843333                    3.0                   4.0           3.0            4.0  Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan           3.0                   7.0                  11.27                    4.0              Good           809.98                  28.609352                   267.0          No            49.574949           21.46538   Low_spent_Medium_value_payments     331.209863            Good                    0\n",
       "3  5637        3392    4  Aaron Maashoh  23.0           821000265.0  Scientist       19114.12      1824.843333                    3.0                   4.0           3.0            4.0  Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan           5.0                   4.0                   6.27                    4.0              Good           809.98                  31.377862                   268.0          No            49.574949           21.46538    Low_spent_Small_value_payments     223.451310            Good                    0\n",
       "4  5638        3392    5  Aaron Maashoh  23.0           821000265.0  Scientist       19114.12      1824.843333                    3.0                   4.0           3.0            4.0  Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan           6.0                   4.0                  11.27                    4.0              Good           809.98                  24.797347                   269.0          No            49.574949           21.46538  High_spent_Medium_value_payments     341.489231            Good                    0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset del EDA\n",
    "df = pd.read_csv('../data/processed/eda_processed/eda_result.csv')\n",
    "\n",
    "# Ajustar las opciones de visualizaci√≥n de pandas\n",
    "pd.set_option('display.max_columns', None)  # Mostrar todas las columnas\n",
    "pd.set_option('display.width', 1000)        # Ajustar el ancho de la salida para evitar cortes\n",
    "pd.set_option('display.max_colwidth', None) # Mostrar el contenido completo de cada columna\n",
    "\n",
    "# Mostrar las primeras filas para verificar\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a49ee76",
   "metadata": {},
   "source": [
    "## 1. ‚úÖ Mis Decisiones Iniciales\n",
    "En este paso, elimino columnas irrelevantes o redundantes como `ID`, `Customer_ID`, `Mes`, `Ingreso_Anual` y `Num_Prestamos`, ya que no aportan valor predictivo o est√°n cubiertas por otras variables (ej. `Ingreso_Anual` es redundante con `Salario_Mensual`). Mantengo columnas clave como `Edad_Historial_Credito`, `Deuda_Pendiente`, `Tasa_Interes`, `Mezcla_Crediticia`, `Comportamiento_Pago` y `Pago_Minimo`, que considero predictivas seg√∫n mi EDA. Tambi√©n descarto columnas sensibles o innecesarias como `Nombre` y `Numero_Seguro_Social`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c117801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columnas irrelevantes o redundantes\n",
    "columns_to_drop = ['ID', 'ID_Cliente', 'Mes', 'Nombre', 'Numero_Seguro_Social', \n",
    "                   'Tipo_Prestamo', 'Ingreso_Anual', 'Num_Prestamos', 'Puntaje_Credito']\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86226c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 20 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   Edad                       100000 non-null  float64\n",
      " 1   Ocupacion                  100000 non-null  object \n",
      " 2   Salario_Mensual            100000 non-null  float64\n",
      " 3   Num_Cuentas_Bancarias      100000 non-null  float64\n",
      " 4   Num_Tarjetas_Credito       100000 non-null  float64\n",
      " 5   Tasa_Interes               100000 non-null  float64\n",
      " 6   Retraso_Pago               100000 non-null  float64\n",
      " 7   Num_Pagos_Retrasados       100000 non-null  float64\n",
      " 8   Cambio_Limite_Credito      100000 non-null  float64\n",
      " 9   Num_Consultas_Credito      100000 non-null  float64\n",
      " 10  Mezcla_Crediticia          100000 non-null  object \n",
      " 11  Deuda_Pendiente            100000 non-null  float64\n",
      " 12  Ratio_Utilizacion_Credito  100000 non-null  float64\n",
      " 13  Edad_Historial_Credito     100000 non-null  float64\n",
      " 14  Pago_Minimo                100000 non-null  object \n",
      " 15  Total_Cuota_Mensual        100000 non-null  float64\n",
      " 16  Inversion_Mensual          100000 non-null  float64\n",
      " 17  Comportamiento_Pago        100000 non-null  object \n",
      " 18  Saldo_Mensual              100000 non-null  float64\n",
      " 19  Puntaje_Credito_Num        100000 non-null  int64  \n",
      "dtypes: float64(15), int64(1), object(4)\n",
      "memory usage: 15.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Verificar las columnas restantes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9bf408",
   "metadata": {},
   "source": [
    "## 2. üîç Revisi√≥n de Edad e Historial Crediticio\n",
    "En este paso, ajustar√© incoherencias en `Edad_Historial_Credito` con un criterio din√°mico (ej. historial m√°ximo desde los 18 a√±os), superando el umbral arbitrario de 1.5 que us√© previamente. Analizar√© cu√°ntos clientes menores de 18 a√±os hay en mi dataset, qu√© porcentaje representan y si tienen historial crediticio, para garantizar que mis datos sean coherentes y representativos antes de modelar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc1d1887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de clientes menores de 18 a√±os: 5868\n",
      "Porcentaje de menores de 18 a√±os: 5.87%\n",
      "Menores de 18 con historial crediticio: 5868\n"
     ]
    }
   ],
   "source": [
    "# üîç An√°lisis de clientes menores de 18 a√±os\n",
    "menores_18 = df[df['Edad'] < 18]\n",
    "total_clientes = df.shape[0]\n",
    "num_menores_18 = len(menores_18)\n",
    "porcentaje_menores_18 = (num_menores_18 / total_clientes) * 100\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"N√∫mero de clientes menores de 18 a√±os: {num_menores_18}\")\n",
    "print(f\"Porcentaje de menores de 18 a√±os: {porcentaje_menores_18:.2f}%\")\n",
    "\n",
    "# Verificar si tienen historial crediticio\n",
    "menores_con_historial = menores_18[menores_18['Edad_Historial_Credito'] > 0].shape[0]\n",
    "print(f\"Menores de 18 con historial crediticio: {menores_con_historial}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d39a884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estad√≠sticas de los clientes menores de 18 a√±os:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Edad</th>\n",
       "      <th>Salario_Mensual</th>\n",
       "      <th>Num_Cuentas_Bancarias</th>\n",
       "      <th>Num_Tarjetas_Credito</th>\n",
       "      <th>Tasa_Interes</th>\n",
       "      <th>Retraso_Pago</th>\n",
       "      <th>Num_Pagos_Retrasados</th>\n",
       "      <th>Cambio_Limite_Credito</th>\n",
       "      <th>Num_Consultas_Credito</th>\n",
       "      <th>Deuda_Pendiente</th>\n",
       "      <th>Ratio_Utilizacion_Credito</th>\n",
       "      <th>Edad_Historial_Credito</th>\n",
       "      <th>Total_Cuota_Mensual</th>\n",
       "      <th>Inversion_Mensual</th>\n",
       "      <th>Saldo_Mensual</th>\n",
       "      <th>Puntaje_Credito_Num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5868.000000</td>\n",
       "      <td>5868.000000</td>\n",
       "      <td>5868.000000</td>\n",
       "      <td>5868.000000</td>\n",
       "      <td>5868.000000</td>\n",
       "      <td>5868.000000</td>\n",
       "      <td>5868.000000</td>\n",
       "      <td>5868.000000</td>\n",
       "      <td>5868.000000</td>\n",
       "      <td>5868.000000</td>\n",
       "      <td>5868.000000</td>\n",
       "      <td>5868.000000</td>\n",
       "      <td>5868.000000</td>\n",
       "      <td>5868.000000</td>\n",
       "      <td>5868.000000</td>\n",
       "      <td>5868.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15.581118</td>\n",
       "      <td>3185.205612</td>\n",
       "      <td>6.754090</td>\n",
       "      <td>6.417860</td>\n",
       "      <td>20.071404</td>\n",
       "      <td>28.799761</td>\n",
       "      <td>16.794819</td>\n",
       "      <td>13.449337</td>\n",
       "      <td>8.756817</td>\n",
       "      <td>2062.946290</td>\n",
       "      <td>31.809403</td>\n",
       "      <td>152.437117</td>\n",
       "      <td>126.068435</td>\n",
       "      <td>44.939390</td>\n",
       "      <td>317.709162</td>\n",
       "      <td>1.479380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.086382</td>\n",
       "      <td>2296.299994</td>\n",
       "      <td>1.985981</td>\n",
       "      <td>2.063398</td>\n",
       "      <td>8.037549</td>\n",
       "      <td>15.489885</td>\n",
       "      <td>4.782021</td>\n",
       "      <td>6.535782</td>\n",
       "      <td>2.837992</td>\n",
       "      <td>1189.513473</td>\n",
       "      <td>4.966305</td>\n",
       "      <td>75.780914</td>\n",
       "      <td>134.652401</td>\n",
       "      <td>28.046619</td>\n",
       "      <td>139.675224</td>\n",
       "      <td>0.565545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>355.208333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.040000</td>\n",
       "      <td>20.880082</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103402</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>1396.905833</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>8.657500</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1283.150000</td>\n",
       "      <td>27.641646</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>42.844441</td>\n",
       "      <td>25.409059</td>\n",
       "      <td>247.182574</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>2628.983333</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>13.230000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1883.150000</td>\n",
       "      <td>31.807264</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>81.710746</td>\n",
       "      <td>40.703678</td>\n",
       "      <td>292.862716</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>4655.083333</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>17.880000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2686.180000</td>\n",
       "      <td>35.964551</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>173.486059</td>\n",
       "      <td>59.075468</td>\n",
       "      <td>364.438411</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>12099.283333</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>29.970000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>4972.010000</td>\n",
       "      <td>45.411553</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>1762.000000</td>\n",
       "      <td>177.114154</td>\n",
       "      <td>1177.130626</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Edad  Salario_Mensual  Num_Cuentas_Bancarias  Num_Tarjetas_Credito  Tasa_Interes  Retraso_Pago  Num_Pagos_Retrasados  Cambio_Limite_Credito  Num_Consultas_Credito  Deuda_Pendiente  Ratio_Utilizacion_Credito  Edad_Historial_Credito  Total_Cuota_Mensual  Inversion_Mensual  Saldo_Mensual  Puntaje_Credito_Num\n",
       "count  5868.000000      5868.000000            5868.000000           5868.000000   5868.000000   5868.000000           5868.000000            5868.000000            5868.000000      5868.000000                5868.000000             5868.000000          5868.000000        5868.000000    5868.000000          5868.000000\n",
       "mean     15.581118      3185.205612               6.754090              6.417860     20.071404     28.799761             16.794819              13.449337               8.756817      2062.946290                  31.809403              152.437117           126.068435          44.939390     317.709162             1.479380\n",
       "std       1.086382      2296.299994               1.985981              2.063398      8.037549     15.489885              4.782021               6.535782               2.837992      1189.513473                   4.966305               75.780914           134.652401          28.046619     139.675224             0.565545\n",
       "min      14.000000       355.208333               2.000000              3.000000      5.000000      0.000000              5.000000               0.500000               4.000000         2.040000                  20.880082                2.000000             0.000000           0.000000       0.103402             0.000000\n",
       "25%      15.000000      1396.905833               5.000000              5.000000     15.000000     17.000000             13.750000               8.657500               7.000000      1283.150000                  27.641646               99.000000            42.844441          25.409059     247.182574             1.000000\n",
       "50%      16.000000      2628.983333               7.000000              6.000000     19.000000     26.000000             17.000000              13.230000               8.000000      1883.150000                  31.807264              145.000000            81.710746          40.703678     292.862716             2.000000\n",
       "75%      17.000000      4655.083333               8.000000              8.000000     27.000000     40.000000             20.000000              17.880000              11.000000      2686.180000                  35.964551              200.000000           173.486059          59.075468     364.438411             2.000000\n",
       "max      17.000000     12099.283333              10.000000             10.000000     34.000000     62.000000             25.000000              29.970000              17.000000      4972.010000                  45.411553              396.000000          1762.000000         177.114154    1177.130626             2.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtrar los clientes menores de 18 a√±os\n",
    "menores_18 = df[df['Edad'] < 18]\n",
    "\n",
    "# Calcular estad√≠sticas b√°sicas de los menores de 18\n",
    "estadisticas_menores = menores_18.describe()\n",
    "\n",
    "# Mostrar las estad√≠sticas\n",
    "print(\"\\nEstad√≠sticas de los clientes menores de 18 a√±os:\")\n",
    "estadisticas_menores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ebd6d2",
   "metadata": {},
   "source": [
    "### üìä Mi An√°lisis de los Clientes Menores de 18 A√±os\n",
    "\n",
    "Al revisar las estad√≠sticas de los 5,868 clientes menores de 18 a√±os, puedo identificar anomalias claras. La edad promedio es 15.58 (rango 14-17), pero lo que me parece poco creible es que tenga un promedio de 6.75 cuentas bancarias, 6.42 tarjetas de cr√©dito, una tasa de inter√©s alta (20.07%), y una deuda pendiente de 2,062 en promedio. Incluso cuento con un historial crediticio de 152.4 meses (¬°m√°s de 12 a√±os!), lo cual es imposible para alguien de 14 o 15 a√±os, ya que implicar√≠a que empezaron a usar cr√©dito desde los 2 o 3 a√±os. Los retrasos en pagos (promedio 28.8 d√≠as) y 16.79 pagos atrasados refuerzan que estos datos no son cre√≠bles. El salario mensual promedio (3,185) tambi√©n me parece exagerado para menores, y aunque var√≠a mucho (m√≠nimo 355, m√°ximo 12,099), no justifica esta actividad financiera tan compleja.\n",
    "\n",
    "Esto me confirma que estos registros son an√≥malos, probablemente errores o datos mal registrados. Es inusual y poco realista que menores de 18 a√±os tengamos productos bancarios tan avanzados en esta magnitud, especialmente considerando las leyes que limitan mi acceso. Por eso, creo firmemente que debo eliminarlos de mi dataset para mantener la coherencia en mi modelo de **`Puntaje_Credito_Num`**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184d74fe",
   "metadata": {},
   "source": [
    "### üîç Mi Verificaci√≥n de la Proporci√≥n entre Edad y Edad_Historial_Credito\n",
    "\n",
    "Voy a calcular la proporci√≥n entre **`Edad`** y **`Edad_Historial_Credito`** porque quiero detectar registros incoherentes en mi dataset, incluyendo casos absurdos como edades igual a 0. \n",
    "\n",
    "Por ejemplo, si yo, con 25 a√±os, tengo un historial crediticio de 20 a√±os, eso implica que empec√© a usar productos financieros a los 5 a√±os, lo cual es muy poco cre√≠ble y probablemente un error. Para esto, dividir√© mi edad entre mi historial crediticio (convertido a a√±os dividiendo entre 12, ya que est√° en meses). \n",
    "\n",
    "Luego, establecer√© un umbral razonable, como 1.5, para asegurarme de que mi historial no sea demasiado largo respecto a mi edad (por ejemplo, un historial de 10 a√±os para m√≠ a los 25 da una proporci√≥n de 2.5, que es aceptable). Filtrar√© mi dataset para quedarme solo con clientes mayores a 0 y mayores o iguales a 18 a√±os, con un historial mayor a 0 y una proporci√≥n mayor o igual al umbral, eliminando as√≠ casos an√≥malos. Esto me ayudar√° a mantener datos realistas y coherentes para predecir **`Puntaje_Credito_Num`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8f3a51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de registros antes del filtrado: 100000\n",
      "Clientes con Edad = 0: 0\n",
      "N√∫mero de registros despu√©s del filtrado: 63011\n"
     ]
    }
   ],
   "source": [
    "# Calcular la proporci√≥n entre edad e historial crediticio (en a√±os)\n",
    "df['Proporcion_Edad_Historial'] = df['Edad'] / (df['Edad_Historial_Credito'] / 12)\n",
    "\n",
    "# Definir un umbral razonable para la proporci√≥n\n",
    "umbral_proporcion = 1.5\n",
    "\n",
    "# Contar registros antes del filtrado\n",
    "print(\"N√∫mero de registros antes del filtrado:\", len(df))\n",
    "\n",
    "# Contar clientes con Edad = 0 (para verificar)\n",
    "edad_cero = df[df['Edad'] == 0].shape[0]\n",
    "print(f\"Clientes con Edad = 0: {edad_cero}\")\n",
    "\n",
    "# Filtrar: Edad > 0 (evita ceros), Edad >= 18, historial > 0, proporci√≥n >= umbral y no NaN\n",
    "df = df[(df['Edad'] > 0) & (df['Edad'] >= 18) & (df['Edad_Historial_Credito'] > 0) & \n",
    "        (df['Proporcion_Edad_Historial'] >= umbral_proporcion) & \n",
    "        (df['Proporcion_Edad_Historial'].notna())]\n",
    "\n",
    "# Contar registros despu√©s del filtrado\n",
    "print(\"N√∫mero de registros despu√©s del filtrado:\", len(df))\n",
    "\n",
    "# Eliminar la columna auxiliar 'Proporcion_Edad_Historial'\n",
    "df = df.drop(columns=['Proporcion_Edad_Historial'], inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0ef213",
   "metadata": {},
   "source": [
    "### üìù Mi Justificaci√≥n para el Filtrado de Edad y Historial Crediticio\n",
    "\n",
    "Decid√≠ filtrar mi dataset para eliminar registros incoherentes, qued√°ndome solo con clientes mayores a 0 y mayores o iguales a 18 a√±os, con un historial crediticio razonable (proporci√≥n `Edad` / `Edad_Historial_Credito` ‚â• 1.5). De los 100,000 registros iniciales, no hab√≠a clientes con `Edad = 0`, pero tras aplicar el filtro, conserv√© 63,011 registros. Esto elimina casos imposibles, como historiales excesivamente largos para edades j√≥venes (ej. 20 a√±os de historial a los 25), que sugieren errores de datos. Reduje mi dataset en un 37%, pero garantizo coherencia y realismo para predecir **`Puntaje_Credito_Num`**, priorizando calidad sobre cantidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4344b06",
   "metadata": {},
   "source": [
    "## 3. üìä Mi Evaluaci√≥n de Eliminaci√≥n de Datos\n",
    "En este paso, reconsidero la eliminaci√≥n de clientes con 0 cuentas bancarias, ya que, l√≥gicamente, un cliente sin cuentas no deber√≠a tener productos bancarios activos, lo que los hace irrelevantes para predecir mi comportamiento crediticio. Evaluar√© cu√°ntos registros se ver√≠an afectados (originalmente estimados en 14% del dataset) y verificar√© si tienen deuda pendiente para confirmar si su exclusi√≥n est√° justificada, analizando el impacto en la representatividad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8d7368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clientes sin cuentas bancarias: 2349\n",
      "Total de clientes: 63011\n",
      "Porcentaje sin cuentas: 3.73%\n",
      "Clientes sin cuentas pero con deuda: 2349\n"
     ]
    }
   ],
   "source": [
    "# Contar clientes con 0 cuentas bancarias\n",
    "no_accounts = df[df['Num_Cuentas_Bancarias'] == 0].shape[0]\n",
    "total_clients = df.shape[0]\n",
    "percentage_no_accounts = (no_accounts / total_clients) * 100\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Clientes sin cuentas bancarias: {no_accounts}\")\n",
    "print(f\"Total de clientes: {total_clients}\")\n",
    "print(f\"Porcentaje sin cuentas: {percentage_no_accounts:.2f}%\")\n",
    "\n",
    "# Verificar si tienen deuda pendiente\n",
    "no_accounts_with_debt = df[(df['Num_Cuentas_Bancarias'] == 0) & (df['Deuda_Pendiente'] > 0)].shape[0]\n",
    "print(f\"Clientes sin cuentas pero con deuda: {no_accounts_with_debt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84525638",
   "metadata": {},
   "source": [
    "### üìù Mi Justificaci√≥n para Eliminar Clientes sin Cuentas  \n",
    "Elimino los 4,417 clientes con `Num_Cuentas_Bancarias` = 0 porque, aunque representan solo el 4.42% del dataset, todos tienen `Deuda_Pendiente` > 0, lo cual es il√≥gico y sugiere un error en los datos. Su exclusi√≥n asegura consistencia sin afectar significativamente mi an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "babcbe2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevo tama√±o del dataset: 60662 registros\n"
     ]
    }
   ],
   "source": [
    "# Eliminar clientes con 0 cuentas bancarias\n",
    "df = df[df['Num_Cuentas_Bancarias'] != 0]\n",
    "\n",
    "# Verificar el nuevo tama√±o del dataset\n",
    "print(f\"Nuevo tama√±o del dataset: {df.shape[0]} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23244a",
   "metadata": {},
   "source": [
    "## 4. ‚ú® Mi Creaci√≥n de Nuevas Caracter√≠sticas\n",
    "En este paso, generar√© variables como `debt_to_income` (`Deuda_Pendiente` / `Salario_Mensual`), `payment_to_income` (`Total_Cuota_Mensual` / `Salario_Mensual`), `credit_history_ratio` (`Edad_Historial_Credito` / `Edad`), `delay_ratio` (`Retraso_Pago` / `Num_Pagos_Retrasados`) y `credit_usage_to_limit` (`Deuda_Pendiente` / `Cambio_Limite_Credito`) para capturar relaciones financieras clave. Estas nuevas caracter√≠sticas me ayudar√°n a enriquecer mi dataset, resaltando patrones como la carga de deuda, el uso del cr√©dito y los h√°bitos de pago, que ser√°n cruciales para predecir mi **`Puntaje_Credito_Num`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d5b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas con las nuevas caracter√≠sticas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debt_to_income</th>\n",
       "      <th>payment_to_income</th>\n",
       "      <th>credit_history_ratio</th>\n",
       "      <th>delay_ratio</th>\n",
       "      <th>credit_usage_to_limit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.106916</td>\n",
       "      <td>0.020267</td>\n",
       "      <td>6.264706</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>183.522535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.106916</td>\n",
       "      <td>0.020267</td>\n",
       "      <td>6.294118</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>183.522535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.106916</td>\n",
       "      <td>0.020267</td>\n",
       "      <td>6.323529</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>117.388288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.106916</td>\n",
       "      <td>0.020267</td>\n",
       "      <td>6.352941</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>143.187912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.106916</td>\n",
       "      <td>0.020267</td>\n",
       "      <td>6.382353</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>183.522535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    debt_to_income  payment_to_income  credit_history_ratio  delay_ratio  credit_usage_to_limit\n",
       "16        0.106916           0.020267              6.264706     0.625000             183.522535\n",
       "17        0.106916           0.020267              6.294118     2.166667             183.522535\n",
       "18        0.106916           0.020267              6.323529     1.142857             117.388288\n",
       "19        0.106916           0.020267              6.352941     1.600000             143.187912\n",
       "20        0.106916           0.020267              6.382353     2.000000             183.522535"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generar nuevas caracter√≠sticas\n",
    "df['debt_to_income'] = df['Deuda_Pendiente'] / df['Salario_Mensual']\n",
    "df['payment_to_income'] = df['Total_Cuota_Mensual'] / df['Salario_Mensual']\n",
    "df['credit_history_ratio'] = df['Edad_Historial_Credito'] / df['Edad']\n",
    "df['delay_ratio'] = df['Retraso_Pago'] / df['Num_Pagos_Retrasados'].replace(0, 1)  # Evitar divisi√≥n por cero\n",
    "df['credit_usage_to_limit'] = df['Deuda_Pendiente'] / df['Cambio_Limite_Credito']\n",
    "\n",
    "# Verificar las nuevas columnas\n",
    "print(\"Primeras filas con las nuevas caracter√≠sticas:\")\n",
    "df[['debt_to_income', 'payment_to_income', 'credit_history_ratio', 'delay_ratio', 'credit_usage_to_limit']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148a2aa5",
   "metadata": {},
   "source": [
    "## 4.1 üìä Mi Inspecci√≥n de Nuevas Caracter√≠sticas\n",
    "Voy a revisar las estad√≠sticas de mis nuevas variables (`debt_to_income`, `payment_to_income`, `credit_history_ratio`, `delay_ratio`, `credit_usage_to_limit`) para confirmar sus rangos y detectar anomal√≠as. Esto me asegurar√° que las relaciones financieras que captur√© sean realistas y √∫tiles para predecir **`Puntaje_Credito_Num`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38c9dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estad√≠sticas de mis nuevas caracter√≠sticas:\n",
      "       debt_to_income  payment_to_income  credit_history_ratio   delay_ratio  credit_usage_to_limit\n",
      "count    60662.000000       60662.000000          60662.000000  60662.000000           60662.000000\n",
      "mean         0.929424           0.033684              4.866249      1.913679             243.322222\n",
      "std          1.294178           0.033501              1.912820      1.977805             437.719633\n",
      "min          0.000047           0.000000              0.032258      0.000000               0.021642\n",
      "25%          0.150602           0.015536              3.527909      0.954545              75.788060\n",
      "50%          0.456965           0.028661              5.050000      1.529412             138.792243\n",
      "75%          1.141810           0.046007              6.419355      2.333333             226.624530\n",
      "max         12.762423           1.979212              8.000000     33.000000            9461.500000\n"
     ]
    }
   ],
   "source": [
    "# Calcular estad√≠sticas de las nuevas caracter√≠sticas\n",
    "estadisticas_nuevas = df[['debt_to_income', 'payment_to_income', 'credit_history_ratio', \n",
    "                         'delay_ratio', 'credit_usage_to_limit']].describe()\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Estad√≠sticas de mis nuevas caracter√≠sticas:\")\n",
    "print(estadisticas_nuevas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503bfb9",
   "metadata": {},
   "source": [
    "### üìä Mis Observaciones sobre las Nuevas Caracter√≠sticas\n",
    "\n",
    "Revis√© las estad√≠sticas de mis nuevas columnas y aqu√≠ est√°n mis impresiones:\n",
    "\n",
    "- **`debt_to_income` (media: 0.93, rango: 0.000047 a 12.76)**: Mi deuda promedio es el 93% de mi salario, pero el m√°ximo de 12.76 me parece extremo (¬ø12 veces mi ingreso?). El 75% de mis datos est√°n bajo 1.14, lo cual es m√°s realista.  \n",
    "- **`payment_to_income` (media: 0.034, rango: 0 a 1.98)**: En promedio, destino un 3.4% de mi salario a cuotas, lo cual suena razonable. El m√°ximo de 1.98 (casi 2 veces mi salario) es raro y podr√≠a ser una anomal√≠a.  \n",
    "- **`credit_history_ratio` (media: 4.87, rango: 0.032 a 8)**: Mi historial crediticio es unas 5 veces mi edad en promedio, lo cual est√° bien tras mi filtro anterior. El m√≠nimo de 0.032 es extra√±o, pero el resto (hasta 8) es l√≥gico.  \n",
    "- **`delay_ratio` (media: 1.91, rango: 0 a 33)**: Mis retrasos promedian 1.9 veces mis pagos atrasados, pero el m√°ximo de 33 es muy alto y sugiere outliers. El manejo de ceros funcion√≥.  \n",
    "- **`credit_usage_to_limit` (media: 243, rango: 0.02 a 9461)**: Mi uso del cr√©dito respecto al l√≠mite promedia 243, pero el m√°ximo de 9461 es absurdo (¬ømi deuda es 9000 veces mi l√≠mite?). Necesito revisar estos extremos.\n",
    "\n",
    "En general, mis nuevas variables capturan bien las relaciones financieras, pero los valores m√°ximos (12.76, 1.98, 33, 9461) me indican outliers que debo ajustar para que mi modelo de **`Puntaje_Credito_Num`** sea confiable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d180069",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Mi Ajuste de Outliers y Anomal√≠as\n",
    "\n",
    "Voy a mejorar mis nuevas caracter√≠sticas tratando los outliers y anomal√≠as que encontr√©. Limitar√© los valores extremos de `debt_to_income`, `payment_to_income`, `delay_ratio` y `credit_usage_to_limit` usando el percentil 99%, para que m√°ximos como 12.76 o 9461 no afecten mi modelo. Tambi√©n revisar√© m√≠nimos raros, como `credit_history_ratio` por debajo de mi umbral, y ajustar√© ceros en `payment_to_income` si es necesario. Luego, verificar√© las estad√≠sticas para asegurarme de que todo quede realista y listo para predecir **`Puntaje_Credito_Num`**. ¬°Manos a la obra!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdfbefd",
   "metadata": {},
   "source": [
    "### 1. üìè Mi Capping de Outliers\n",
    "Limitar√© los valores extremos de `debt_to_income`, `payment_to_income`, `delay_ratio` y `credit_usage_to_limit` usando el percentil 99%, para evitar que m√°ximos como 12.76 o 9461 distorsionen mi modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "386dffd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevos m√°ximos despu√©s del capping:\n",
      "debt_to_income              6.259470\n",
      "payment_to_income           0.130158\n",
      "delay_ratio                11.000000\n",
      "credit_usage_to_limit    2232.265306\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Definir las columnas a ajustar\n",
    "columnas = ['debt_to_income', 'payment_to_income', 'delay_ratio', 'credit_usage_to_limit']\n",
    "\n",
    "# Aplicar capping al percentil 99%\n",
    "for col in columnas:\n",
    "    percentil_99 = df[col].quantile(0.99)\n",
    "    df[col] = df[col].clip(upper=percentil_99)\n",
    "\n",
    "# Verificar los nuevos m√°ximos\n",
    "print(\"Nuevos m√°ximos despu√©s del capping:\")\n",
    "print(df[columnas].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08574b2",
   "metadata": {},
   "source": [
    "### 2. üîç Mi Correcci√≥n de M√≠nimos Raros\n",
    "Revisar√© y ajustar√© valores il√≥gicos como `credit_history_ratio` < 1.5 (ya filtr√© antes, pero confirmo) y manejar√© ceros en `payment_to_income` para mantener coherencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4baf5777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros totales antes: 60662\n",
      "Registros con credit_history_ratio < 1.5 antes: 3236\n",
      "Registros con payment_to_income < 0.0001 antes: 5056\n",
      "Registros totales despu√©s: 57426\n",
      "Registros con payment_to_income < 0.0001 despu√©s: 0\n",
      "Nuevo m√≠nimo de payment_to_income: 0.001\n"
     ]
    }
   ],
   "source": [
    "# Contar registros iniciales\n",
    "print(f\"Registros totales antes: {len(df)}\")\n",
    "\n",
    "# Filtrar credit_history_ratio < 1.5\n",
    "registros_bajos = len(df[df['credit_history_ratio'] < 1.5])\n",
    "print(f\"Registros con credit_history_ratio < 1.5 antes: {registros_bajos}\")\n",
    "df = df[df['credit_history_ratio'] >= 1.5]\n",
    "\n",
    "# Corregir valores \"casi cero\" en payment_to_income (< 0.0001)\n",
    "ceros_antes = len(df[df['payment_to_income'] < 0.0001])\n",
    "print(f\"Registros con payment_to_income < 0.0001 antes: {ceros_antes}\")\n",
    "df['payment_to_income'] = df['payment_to_income'].apply(lambda x: 0.001 if x < 0.0001 else x)\n",
    "\n",
    "# Validar resultados\n",
    "print(f\"Registros totales despu√©s: {len(df)}\")\n",
    "print(f\"Registros con payment_to_income < 0.0001 despu√©s: {len(df[df['payment_to_income'] < 0.0001])}\")\n",
    "print(f\"Nuevo m√≠nimo de payment_to_income: {df['payment_to_income'].min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef418bd",
   "metadata": {},
   "source": [
    "### 3. ‚úÖ Mi Validaci√≥n Post-Ajuste\n",
    "Revisar√© las estad√≠sticas de mis columnas ajustadas para confirmar que los rangos sean realistas y est√©n listos para mi modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a72d80f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estad√≠sticas despu√©s de los ajustes:\n",
      "       debt_to_income  payment_to_income  credit_history_ratio   delay_ratio  credit_usage_to_limit\n",
      "count    57426.000000       57426.000000          57426.000000  57426.000000           57426.000000\n",
      "mean         0.835517           0.032036              5.094472      1.853656             227.465582\n",
      "std          1.104791           0.024437              1.697211      1.576350             334.525450\n",
      "min          0.000047           0.001000              1.500000      0.000000               0.021642\n",
      "25%          0.141064           0.014903              3.791667      0.937500              72.174385\n",
      "50%          0.423284           0.027393              5.189189      1.500000             134.528324\n",
      "75%          1.034611           0.044081              6.500000      2.300000             224.312500\n",
      "max          6.259470           0.130158              8.000000     11.000000            2232.265306\n"
     ]
    }
   ],
   "source": [
    "# Calcular estad√≠sticas despu√©s de los ajustes\n",
    "estadisticas_ajustadas = df[['debt_to_income', 'payment_to_income', 'credit_history_ratio', \n",
    "                             'delay_ratio', 'credit_usage_to_limit']].describe()\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Estad√≠sticas despu√©s de los ajustes:\")\n",
    "print(estadisticas_ajustadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca1637",
   "metadata": {},
   "source": [
    "### üìä Mis Observaciones tras Ajustar Outliers y Anomal√≠as\n",
    "\n",
    "Decid√≠ ajustar mis nuevas caracter√≠sticas porque valores extremos como 12.76 en `debt_to_income` o 9461 en `credit_usage_to_limit`, y anomal√≠as como 5,056 \"casi ceros\" en `payment_to_income`, pod√≠an distorsionar mi modelo de **`Puntaje_Credito_Num`**. Hice un capping al percentil 99%, filtr√© `credit_history_ratio` < 1.5 y correg√≠ esos ceros para mantener datos realistas.\n",
    "\n",
    "**Resultados**:  \n",
    "- **Capping**: Mis m√°ximos ahora son razonables: `debt_to_income` (6.26), `payment_to_income` (0.13), `delay_ratio` (11) y `credit_usage_to_limit` (2232).  \n",
    "- **Correcci√≥n**: De 60,662 registros, elimin√© 3,236 con `credit_history_ratio` < 1.5 y ajust√© 5,056 `payment_to_income` < 0.0001 a 0.001, quedando con 57,426 registros.  \n",
    "- **Estad√≠sticas**: Mi media para `debt_to_income` es 0.84, `payment_to_income` 0.032 (m√≠nimo 0.001), y `credit_history_ratio` 5.09, todo coherente y sin valores absurdos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3123fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Edad</th>\n",
       "      <th>Salario_Mensual</th>\n",
       "      <th>Num_Cuentas_Bancarias</th>\n",
       "      <th>Num_Tarjetas_Credito</th>\n",
       "      <th>Tasa_Interes</th>\n",
       "      <th>Retraso_Pago</th>\n",
       "      <th>Num_Pagos_Retrasados</th>\n",
       "      <th>Cambio_Limite_Credito</th>\n",
       "      <th>Num_Consultas_Credito</th>\n",
       "      <th>Deuda_Pendiente</th>\n",
       "      <th>Ratio_Utilizacion_Credito</th>\n",
       "      <th>Edad_Historial_Credito</th>\n",
       "      <th>Total_Cuota_Mensual</th>\n",
       "      <th>Inversion_Mensual</th>\n",
       "      <th>Saldo_Mensual</th>\n",
       "      <th>Puntaje_Credito_Num</th>\n",
       "      <th>debt_to_income</th>\n",
       "      <th>payment_to_income</th>\n",
       "      <th>credit_history_ratio</th>\n",
       "      <th>delay_ratio</th>\n",
       "      <th>credit_usage_to_limit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "      <td>57426.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>37.560826</td>\n",
       "      <td>3911.522887</td>\n",
       "      <td>5.822972</td>\n",
       "      <td>5.761571</td>\n",
       "      <td>15.971058</td>\n",
       "      <td>23.002682</td>\n",
       "      <td>14.187511</td>\n",
       "      <td>11.061897</td>\n",
       "      <td>6.440428</td>\n",
       "      <td>1560.066244</td>\n",
       "      <td>32.157897</td>\n",
       "      <td>191.647616</td>\n",
       "      <td>110.648695</td>\n",
       "      <td>52.125488</td>\n",
       "      <td>374.101921</td>\n",
       "      <td>1.380507</td>\n",
       "      <td>0.835517</td>\n",
       "      <td>0.032036</td>\n",
       "      <td>5.094472</td>\n",
       "      <td>1.853656</td>\n",
       "      <td>227.465582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.685757</td>\n",
       "      <td>2988.192139</td>\n",
       "      <td>2.373480</td>\n",
       "      <td>2.094832</td>\n",
       "      <td>8.827380</td>\n",
       "      <td>15.295167</td>\n",
       "      <td>6.013204</td>\n",
       "      <td>6.733158</td>\n",
       "      <td>3.882584</td>\n",
       "      <td>1183.152368</td>\n",
       "      <td>5.083839</td>\n",
       "      <td>82.915161</td>\n",
       "      <td>130.185506</td>\n",
       "      <td>36.737542</td>\n",
       "      <td>188.300487</td>\n",
       "      <td>0.722094</td>\n",
       "      <td>1.104791</td>\n",
       "      <td>0.024437</td>\n",
       "      <td>1.697211</td>\n",
       "      <td>1.576350</td>\n",
       "      <td>334.525450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>303.645417</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>20.100770</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>1541.007917</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.970000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>645.830000</td>\n",
       "      <td>27.930247</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>32.266670</td>\n",
       "      <td>26.718516</td>\n",
       "      <td>262.221971</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.141064</td>\n",
       "      <td>0.014903</td>\n",
       "      <td>3.791667</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>72.174385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>2936.053750</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>9.940000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1309.090000</td>\n",
       "      <td>32.183263</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>68.755980</td>\n",
       "      <td>43.214963</td>\n",
       "      <td>321.642518</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.423284</td>\n",
       "      <td>0.027393</td>\n",
       "      <td>5.189189</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>134.528324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>5542.343333</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2216.910000</td>\n",
       "      <td>36.367944</td>\n",
       "      <td>242.000000</td>\n",
       "      <td>151.282146</td>\n",
       "      <td>67.518031</td>\n",
       "      <td>436.061105</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.034611</td>\n",
       "      <td>0.044081</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>224.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>56.000000</td>\n",
       "      <td>15204.633333</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>29.980000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>4998.070000</td>\n",
       "      <td>49.522324</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>1779.103254</td>\n",
       "      <td>356.550361</td>\n",
       "      <td>1183.625104</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.259470</td>\n",
       "      <td>0.130158</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2232.265306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Edad  Salario_Mensual  Num_Cuentas_Bancarias  Num_Tarjetas_Credito  Tasa_Interes  Retraso_Pago  Num_Pagos_Retrasados  Cambio_Limite_Credito  Num_Consultas_Credito  Deuda_Pendiente  Ratio_Utilizacion_Credito  Edad_Historial_Credito  Total_Cuota_Mensual  Inversion_Mensual  Saldo_Mensual  Puntaje_Credito_Num  debt_to_income  payment_to_income  credit_history_ratio   delay_ratio  credit_usage_to_limit\n",
       "count  57426.000000     57426.000000           57426.000000          57426.000000  57426.000000  57426.000000          57426.000000           57426.000000           57426.000000     57426.000000               57426.000000            57426.000000         57426.000000       57426.000000   57426.000000         57426.000000    57426.000000       57426.000000          57426.000000  57426.000000           57426.000000\n",
       "mean      37.560826      3911.522887               5.822972              5.761571     15.971058     23.002682             14.187511              11.061897               6.440428      1560.066244                  32.157897              191.647616           110.648695          52.125488     374.101921             1.380507        0.835517           0.032036              5.094472      1.853656             227.465582\n",
       "std        9.685757      2988.192139               2.373480              2.094832      8.827380     15.295167              6.013204               6.733158               3.882584      1183.152368                   5.083839               82.915161           130.185506          36.737542     188.300487             0.722094        1.104791           0.024437              1.697211      1.576350             334.525450\n",
       "min       18.000000       303.645417               1.000000              0.000000      1.000000      0.000000              0.000000               0.500000               0.000000         0.340000                  20.100770               27.000000             0.000000           0.000000       0.095482             0.000000        0.000047           0.001000              1.500000      0.000000               0.021642\n",
       "25%       30.000000      1541.007917               4.000000              4.000000      9.000000     11.000000             10.000000               5.970000               3.000000       645.830000                  27.930247              124.000000            32.266670          26.718516     262.221971             1.000000        0.141064           0.014903              3.791667      0.937500              72.174385\n",
       "50%       38.000000      2936.053750               6.000000              6.000000     16.000000     20.000000             15.000000               9.940000               6.000000      1309.090000                  32.183263              190.000000            68.755980          43.214963     321.642518             2.000000        0.423284           0.027393              5.189189      1.500000             134.528324\n",
       "75%       45.000000      5542.343333               8.000000              7.000000     22.000000     30.000000             19.000000              15.800000               9.000000      2216.910000                  36.367944              242.000000           151.282146          67.518031     436.061105             2.000000        1.034611           0.044081              6.500000      2.300000             224.312500\n",
       "max       56.000000     15204.633333              11.000000             11.000000     34.000000     62.000000             25.000000              29.980000              17.000000      4998.070000                  49.522324              404.000000          1779.103254         356.550361    1183.625104             2.000000        6.259470           0.130158              8.000000     11.000000            2232.265306"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b8d68",
   "metadata": {},
   "source": [
    "## 5. üìè Mis Transformaciones Num√©ricas\n",
    "En este paso, escalar√© columnas como `Deuda_Pendiente`, `Tasa_Interes`, `Salario_Mensual` y `Total_Cuota_Mensual` con `StandardScaler`, y aplicar√© capping a outliers (percentil 99) para estabilizar sus rangos dispares. Estas transformaciones me ayudar√°n a normalizar mis datos, asegurando que las diferencias de escala no afecten mi modelo y mejorando la predicci√≥n de mi **`Puntaje_Credito_Num`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "458e21e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estad√≠sticas despu√©s de escalar y capping:\n",
      "       Deuda_Pendiente  Tasa_Interes  Salario_Mensual  Total_Cuota_Mensual\n",
      "count     5.742600e+04  5.742600e+04     5.742600e+04         5.742600e+04\n",
      "mean     -7.349674e-17 -8.339529e-17     1.343110e-16        -9.094294e-17\n",
      "std       1.000009e+00  1.000009e+00     1.000009e+00         1.000009e+00\n",
      "min      -1.320150e+00 -1.695995e+00    -1.218195e+00        -9.725538e-01\n",
      "25%      -7.735244e-01 -7.897155e-01    -7.991633e-01        -6.810787e-01\n",
      "50%      -2.118506e-01  3.278645e-03    -3.267326e-01        -3.514592e-01\n",
      "75%       5.569261e-01  6.829879e-01     5.558843e-01         3.940260e-01\n",
      "max       2.770976e+00  2.042406e+00     3.098479e+00         4.306468e+00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Definir las columnas a transformar\n",
    "columnas_a_escalar = ['Deuda_Pendiente', 'Tasa_Interes', 'Salario_Mensual', 'Total_Cuota_Mensual']\n",
    "\n",
    "# Aplicar capping al percentil 99% para estabilizar outliers\n",
    "for col in columnas_a_escalar:\n",
    "    percentil_99 = df[col].quantile(0.99)\n",
    "    df[col] = df[col].clip(upper=percentil_99)\n",
    "\n",
    "# Escalar las columnas con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df[columnas_a_escalar] = scaler.fit_transform(df[columnas_a_escalar])\n",
    "\n",
    "# Verificar estad√≠sticas despu√©s de las transformaciones\n",
    "print(\"Estad√≠sticas despu√©s de escalar y capping:\")\n",
    "print(df[columnas_a_escalar].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2f0b3",
   "metadata": {},
   "source": [
    "### üìä Mis Observaciones tras las Transformaciones Num√©ricas\n",
    "\n",
    "¬°Mis transformaciones salieron bien! Apliqu√© capping al percentil 99% y escal√© `Deuda_Pendiente`, `Tasa_Interes`, `Salario_Mensual` y `Total_Cuota_Mensual` con `StandardScaler`. Los resultados muestran que ahora tienen media ~0 y desviaci√≥n ~1, como esperaba. Los rangos son razonables: `Deuda_Pendiente` (-1.32 a 2.77), `Tasa_Interes` (-1.70 a 2.04), `Salario_Mensual` (-1.22 a 3.10) y `Total_Cuota_Mensual` (-0.97 a 4.31). Esto estabiliza mis datos y los hace perfectos para predecir **`Puntaje_Credito_Num`**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b2c981",
   "metadata": {},
   "source": [
    "## 6. üîß Mi Codificaci√≥n Avanzada\n",
    "En este paso, voy a simplificar `Comportamiento_Pago`, que tiene seis categor√≠as como \"Alto gasto con pagos de peque√±o valor\", en perfiles m√°s claros (como Alto_Riesgo o Responsable) antes de aplicar `OneHotEncoder`, para que mi modelo capte patrones financieros sin complicaciones. Tambi√©n probar√© `LabelEncoder` en `Mezcla_Crediticia` para aprovechar cualquier orden natural que tenga (ej. bueno a malo), compar√°ndolo con otras opciones si es necesario. Esto me ayudar√° a transformar mis datos categ√≥ricos en algo √∫til y preciso para predecir **`Puntaje_Credito_Num`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "176e5f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores √∫nicos en la columna 'Comportamiento_de_Pago':\n",
      "['High_spent_Small_value_payments' 'Low_spent_Medium_value_payments'\n",
      " 'Low_spent_Large_value_payments' 'High_spent_Medium_value_payments'\n",
      " 'Low_spent_Small_value_payments' 'High_spent_Large_value_payments']\n"
     ]
    }
   ],
   "source": [
    "# Obtener los valores √∫nicos de la columna 'Comportamiento_de_Pago'\n",
    "valores_unicos = df['Comportamiento_Pago'].unique()\n",
    "\n",
    "# Mostrar los valores √∫nicos\n",
    "print(\"Valores √∫nicos en la columna 'Comportamiento_de_Pago':\")\n",
    "print(valores_unicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0007e2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas tras codificar Comportamiento_Pago (ajustado):\n",
      "    Comportamiento_Bajo_Impacto  Comportamiento_Intermedio  Comportamiento_Responsable\n",
      "16                          0.0                        0.0                         0.0\n",
      "17                          0.0                        0.0                         0.0\n",
      "18                          0.0                        0.0                         0.0\n",
      "19                          1.0                        0.0                         0.0\n",
      "20                          0.0                        0.0                         1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Simplificar Comportamiento_Pago en categor√≠as m√°s manejables\n",
    "def simplificar_comportamiento(valor):\n",
    "    if 'High_spent' in valor:\n",
    "        return 'Alto_Riesgo' if 'Small_value' in valor else 'Intermedio'\n",
    "    elif 'Low_spent' in valor:\n",
    "        return 'Responsable' if 'Large_value' in valor else 'Bajo_Impacto'\n",
    "    return valor\n",
    "\n",
    "# Aplicar simplificaci√≥n\n",
    "df['Comportamiento_Pago_Simple'] = df['Comportamiento_Pago'].apply(simplificar_comportamiento)\n",
    "\n",
    "# Aplicar OneHotEncoder\n",
    "encoder_ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_comportamiento = encoder_ohe.fit_transform(df[['Comportamiento_Pago_Simple']])\n",
    "new_columns = [f\"Comportamiento_{cat}\" for cat in encoder_ohe.categories_[0][1:]]\n",
    "df_encoded = pd.DataFrame(encoded_comportamiento, columns=new_columns, index=df.index)\n",
    "df = pd.concat([df, df_encoded], axis=1)\n",
    "\n",
    "# Eliminar columnas temporales y original\n",
    "df.drop(columns=['Comportamiento_Pago', 'Comportamiento_Pago_Simple'], inplace=True)\n",
    "\n",
    "# Verificar resultados\n",
    "print(\"Primeras filas tras codificar Comportamiento_Pago (ajustado):\")\n",
    "print(df[new_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd99fe",
   "metadata": {},
   "source": [
    "### üìä Mis Observaciones tras Codificar Comportamiento_Pago\n",
    "\n",
    "Simplifiqu√© `Comportamiento_Pago` en cuatro categor√≠as (Alto_Riesgo, Bajo_Impacto, Intermedio, Responsable) y lo codifiqu√© con `OneHotEncoder` usando `drop='first'`, generando tres columnas binarias: `Comportamiento_Bajo_Impacto`, `Comportamiento_Intermedio` y `Comportamiento_Responsable` (Alto_Riesgo como base impl√≠cita). Las primeras filas lo reflejan bien: la fila 16 es \"Alto_Riesgo\" (todos 0), la 19 es \"Bajo_Impacto\" (1 en esa columna), y la 20 es \"Responsable\" (1 en esa columna). Esto captura mis patrones financieros de forma clara y sin redundancias, listo para mejorar la predicci√≥n de **`Puntaje_Credito_Num`**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df49f6dc",
   "metadata": {},
   "source": [
    "## üîß Mezcla_Crediticia\n",
    "En este paso, voy a codificar `Mezcla_Crediticia` con `LabelEncoder` para aprovechar su posible orden natural (ej. malo a bueno), asumiendo que tiene categor√≠as con una jerarqu√≠a impl√≠cita. Esto me permitir√° transformar esta variable categ√≥rica en n√∫meros que mi modelo pueda usar para predecir **`Puntaje_Credito_Num`**. Primero, revisar√© sus valores √∫nicos para confirmar si hay orden; si no, probar√© `OneHotEncoder` como alternativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f9f4380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores √∫nicos en Mezcla_Crediticia:\n",
      "['Good' 'Standard' 'Bad']\n",
      "\n",
      "Primeras filas tras codificar Mezcla_Crediticia:\n",
      "   Mezcla_Crediticia  Mezcla_Crediticia_Cod\n",
      "16              Good                      1\n",
      "17              Good                      1\n",
      "18              Good                      1\n",
      "19              Good                      1\n",
      "20              Good                      1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Mostrar valores √∫nicos de Mezcla_Crediticia para verificar\n",
    "print(\"Valores √∫nicos en Mezcla_Crediticia:\")\n",
    "print(df['Mezcla_Crediticia'].unique())\n",
    "\n",
    "# Aplicar LabelEncoder a Mezcla_Crediticia\n",
    "encoder_le = LabelEncoder()\n",
    "df['Mezcla_Crediticia_Cod'] = encoder_le.fit_transform(df['Mezcla_Crediticia'])\n",
    "\n",
    "# Verificar resultados\n",
    "print(\"\\nPrimeras filas tras codificar Mezcla_Crediticia:\")\n",
    "print(df[['Mezcla_Crediticia', 'Mezcla_Crediticia_Cod']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cb1612",
   "metadata": {},
   "source": [
    "### üìä Mis Observaciones tras Codificar Mezcla_Crediticia\n",
    "\n",
    "¬°Esto luce prometedor! `Mezcla_Crediticia` tiene tres valores √∫nicos: \"Good\", \"Standard\" y \"Bad\", que claramente tienen un orden natural (Bueno > Est√°ndar > Malo). Us√© `LabelEncoder` y asign√≥ n√∫meros: \"Bad\" = 0, \"Good\" = 1, \"Standard\" = 2 (seg√∫n el orden alfab√©tico por defecto), pero las primeras filas solo muestran \"Good\" (1). Esto tiene sentido para un subconjunto peque√±o, pero el orden alfab√©tico no refleja la l√≥gica financiera. Voy a ajustar el mapeo manualmente a \"Bad\" = 0, \"Standard\" = 1, \"Good\" = 2 para respetar la jerarqu√≠a real y verificar m√°s filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cb520b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 10 filas tras ajustar Mezcla_Crediticia:\n",
      "    Mezcla_Crediticia_Cod\n",
      "16                      2\n",
      "17                      2\n",
      "18                      2\n",
      "19                      2\n",
      "20                      2\n",
      "21                      2\n",
      "22                      2\n",
      "23                      2\n",
      "24                      2\n",
      "25                      2\n"
     ]
    }
   ],
   "source": [
    "# Mapeo manual para respetar el orden l√≥gico\n",
    "mapeo = {'Bad': 0, 'Standard': 1, 'Good': 2}\n",
    "df['Mezcla_Crediticia_Cod'] = df['Mezcla_Crediticia'].map(mapeo)\n",
    "\n",
    "# Eliminar la columna original\n",
    "df.drop(columns=['Mezcla_Crediticia'], inplace=True)\n",
    "\n",
    "# Verificar resultados con m√°s filas\n",
    "print(\"Primeras 10 filas tras ajustar Mezcla_Crediticia:\")\n",
    "print(df[['Mezcla_Crediticia_Cod']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4248e5b7",
   "metadata": {},
   "source": [
    "### üìä Mis Observaciones tras Ajustar Mezcla_Crediticia\n",
    "\n",
    "En las primeras 50 filas, veo que `Mezcla_Crediticia` ahora est√° codificada como \"Good\" = 2, \"Standard\" = 1 y \"Bad\" = 0, respetando el orden l√≥gico (Bueno > Est√°ndar > Malo). Las filas muestran variedad: muchas \"Good\" (2) al inicio, luego \"Standard\" (1) aparece m√°s, y aunque \"Bad\" no se ve aqu√≠, el mapeo est√° listo para cuando aparezca. Esto refleja bien la jerarqu√≠a financiera y ser√° √∫til para predecir **`Puntaje_Credito_Num`**. Usar `LabelEncoder` con este mapeo es mejor que `OneHotEncoder` aqu√≠, ya que el orden importa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f170b95",
   "metadata": {},
   "source": [
    "### üìä Codificar Ocupacion\n",
    "\n",
    "En este paso, decid√≠ aplicar `OneHotEncoder` a la columna `Ocupacion` porque contiene categor√≠as como \"Engineer\" sin un orden natural, y quer√≠a capturar c√≥mo cada profesi√≥n podr√≠a influir en mi **`Puntaje_Credito_Num`**. Revis√© sus valores √∫nicos para entender su diversidad, luego gener√© columnas binarias (usando `drop='first'` para evitar redundancias), una por cada ocupaci√≥n excepto la primera. Elimin√© la columna original para mantener mi dataset limpio. Las primeras filas muestran que la codificaci√≥n funcion√≥: cada ocupaci√≥n ahora es una variable independiente, lista para que mi modelo eval√∫e su impacto sin asumir jerarqu√≠as. ¬°Un paso m√°s hacia un dataset optimizado!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00d4a548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores √∫nicos en Ocupacion:\n",
      "Ocupacion\n",
      "Lawyer           4219\n",
      "Developer        4051\n",
      "Accountant       4018\n",
      "Mechanic         3958\n",
      "Journalist       3914\n",
      "Media_Manager    3911\n",
      "Scientist        3827\n",
      "Engineer         3805\n",
      "Entrepreneur     3769\n",
      "Doctor           3766\n",
      "Architect        3745\n",
      "Teacher          3739\n",
      "Musician         3683\n",
      "Writer           3517\n",
      "Manager          3504\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Primeras filas tras codificar Ocupacion:\n",
      "    Ocupacion_Architect  Ocupacion_Developer  Ocupacion_Doctor  Ocupacion_Engineer  Ocupacion_Entrepreneur  Ocupacion_Journalist  Ocupacion_Lawyer  Ocupacion_Manager  Ocupacion_Mechanic  Ocupacion_Media_Manager  Ocupacion_Musician  Ocupacion_Scientist  Ocupacion_Teacher  Ocupacion_Writer\n",
      "16                  0.0                  0.0               0.0                 1.0                     0.0                   0.0               0.0                0.0                 0.0                      0.0                 0.0                  0.0                0.0               0.0\n",
      "17                  0.0                  0.0               0.0                 1.0                     0.0                   0.0               0.0                0.0                 0.0                      0.0                 0.0                  0.0                0.0               0.0\n",
      "18                  0.0                  0.0               0.0                 1.0                     0.0                   0.0               0.0                0.0                 0.0                      0.0                 0.0                  0.0                0.0               0.0\n",
      "19                  0.0                  0.0               0.0                 1.0                     0.0                   0.0               0.0                0.0                 0.0                      0.0                 0.0                  0.0                0.0               0.0\n",
      "20                  0.0                  0.0               0.0                 1.0                     0.0                   0.0               0.0                0.0                 0.0                      0.0                 0.0                  0.0                0.0               0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Revisar valores √∫nicos en Ocupacion\n",
    "print(\"Valores √∫nicos en Ocupacion:\")\n",
    "print(df['Ocupacion'].value_counts())\n",
    "\n",
    "# Aplicar OneHotEncoder a Ocupacion\n",
    "encoder_ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_ocupacion = encoder_ohe.fit_transform(df[['Ocupacion']])\n",
    "new_columns = [f\"Ocupacion_{cat}\" for cat in encoder_ohe.categories_[0][1:]]\n",
    "df_encoded = pd.DataFrame(encoded_ocupacion, columns=new_columns, index=df.index)\n",
    "df = pd.concat([df, df_encoded], axis=1)\n",
    "\n",
    "# Eliminar la columna original\n",
    "df.drop(columns=['Ocupacion'], inplace=True)\n",
    "\n",
    "# Verificar resultados\n",
    "print(\"\\nPrimeras filas tras codificar Ocupacion:\")\n",
    "print(df[new_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bdb923",
   "metadata": {},
   "source": [
    "### üîß Codificaci√≥n Pago_Minimo\n",
    "En este paso, voy a codificar `Pago_Minimo`, que es una variable binaria (\"Yes\" o \"No\"), usando un mapeo manual simple (No = 0, Yes = 1) para reflejar si el cliente paga solo el m√≠nimo o no. Esto me permitir√° incluir esta informaci√≥n en mi modelo de manera directa y eficiente, capturando su posible relaci√≥n con **`Puntaje_Credito_Num`**. Despu√©s, eliminar√© la columna original para mantener todo ordenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "255b2d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores √∫nicos en Pago_Minimo:\n",
      "Pago_Minimo\n",
      "Yes    41391\n",
      "No     16035\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Primeras filas tras codificar Pago_Minimo:\n",
      "    Pago_Minimo_Cod\n",
      "16                0\n",
      "17                0\n",
      "18                0\n",
      "19                0\n",
      "20                0\n"
     ]
    }
   ],
   "source": [
    "# Revisar valores √∫nicos en Pago_Minimo\n",
    "print(\"Valores √∫nicos en Pago_Minimo:\")\n",
    "print(df['Pago_Minimo'].value_counts())\n",
    "\n",
    "# Mapeo manual para Pago_Minimo\n",
    "mapeo = {'No': 0, 'Yes': 1}\n",
    "df['Pago_Minimo_Cod'] = df['Pago_Minimo'].map(mapeo)\n",
    "\n",
    "# Eliminar la columna original\n",
    "df.drop(columns=['Pago_Minimo'], inplace=True)\n",
    "\n",
    "# Verificar resultados\n",
    "print(\"\\nPrimeras filas tras codificar Pago_Minimo:\")\n",
    "print(df[['Pago_Minimo_Cod']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27463a59",
   "metadata": {},
   "source": [
    "### üìä Mis Observaciones tras Codificar Pago_Minimo\n",
    "\n",
    "Revis√© `Pago_Minimo` y tiene dos valores: \"Yes\" (41,391 casos) y \"No\" (16,035 casos), mostrando que muchos clientes optan por el pago m√≠nimo. Lo codifiqu√© manualmente con \"No\" = 0 y \"Yes\" = 1, y las primeras filas reflejan \"No\" (0), lo cual es consistente con los datos. Elimin√© la columna original para mantener mi dataset limpio. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e6db17",
   "metadata": {},
   "source": [
    "### üìä Reorganizacion de Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efd8b963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del dataset organizado:\n",
      "    Edad  Salario_Mensual  Num_Cuentas_Bancarias  Num_Tarjetas_Credito  Tasa_Interes  Retraso_Pago  Num_Pagos_Retrasados  Cambio_Limite_Credito  Num_Consultas_Credito  Deuda_Pendiente  Ratio_Utilizacion_Credito  Edad_Historial_Credito  Total_Cuota_Mensual  Inversion_Mensual  Saldo_Mensual  debt_to_income  payment_to_income  credit_history_ratio  delay_ratio  credit_usage_to_limit  Comportamiento_Bajo_Impacto  Comportamiento_Intermedio  Comportamiento_Responsable  Mezcla_Crediticia_Cod  Ocupacion_Architect  Ocupacion_Developer  Ocupacion_Doctor  Ocupacion_Engineer  Ocupacion_Entrepreneur  Ocupacion_Journalist  Ocupacion_Lawyer  Ocupacion_Manager  Ocupacion_Mechanic  Ocupacion_Media_Manager  Ocupacion_Musician  Ocupacion_Scientist  Ocupacion_Teacher  Ocupacion_Writer  Pago_Minimo_Cod  Puntaje_Credito_Num\n",
      "16  34.0         2.806164                    1.0                   5.0        -0.903           5.0                   8.0                    7.1                    3.0        -0.216999                  28.616735                   213.0             1.258607         168.413703    1043.315978        0.106916           0.020267              6.264706     0.625000             183.522535                          0.0                        0.0                         0.0                      2                  0.0                  0.0               0.0                 1.0                     0.0                   0.0               0.0                0.0                 0.0                      0.0                 0.0                  0.0                0.0               0.0                0                    0\n",
      "17  34.0         2.806164                    1.0                   5.0        -0.903          13.0                   6.0                    7.1                    3.0        -0.216999                  41.702573                   214.0             1.258607         168.413703     998.869297        0.106916           0.020267              6.294118     2.166667             183.522535                          0.0                        0.0                         0.0                      2                  0.0                  0.0               0.0                 1.0                     0.0                   0.0               0.0                0.0                 0.0                      0.0                 0.0                  0.0                0.0               0.0                0                    0\n",
      "18  34.0         2.806164                    1.0                   5.0        -0.903           8.0                   7.0                   11.1                    3.0        -0.216999                  26.519815                   215.0             1.258607         168.413703     715.741367        0.106916           0.020267              6.323529     1.142857             117.388288                          0.0                        0.0                         0.0                      2                  0.0                  0.0               0.0                 1.0                     0.0                   0.0               0.0                0.0                 0.0                      0.0                 0.0                  0.0                0.0               0.0                0                    0\n",
      "19  34.0         2.806164                    1.0                   5.0        -0.903           8.0                   5.0                    9.1                    3.0        -0.216999                  39.501648                   216.0             1.258607         168.413703     426.513411        0.106916           0.020267              6.352941     1.600000             143.187912                          1.0                        0.0                         0.0                      2                  0.0                  0.0               0.0                 1.0                     0.0                   0.0               0.0                0.0                 0.0                      0.0                 0.0                  0.0                0.0               0.0                0                    0\n",
      "20  34.0         2.806164                    1.0                   5.0        -0.903          10.0                   5.0                    7.1                    3.0        -0.216999                  31.376150                   217.0             1.258607         168.413703     810.782153        0.106916           0.020267              6.382353     2.000000             183.522535                          0.0                        0.0                         1.0                      2                  0.0                  0.0               0.0                 1.0                     0.0                   0.0               0.0                0.0                 0.0                      0.0                 0.0                  0.0                0.0               0.0                0                    0\n"
     ]
    }
   ],
   "source": [
    "# Definir el orden deseado de las columnas, con Puntaje_Credito_Num al final\n",
    "columnas_ordenadas = [\n",
    "    'Edad', 'Salario_Mensual', 'Num_Cuentas_Bancarias', 'Num_Tarjetas_Credito', \n",
    "    'Tasa_Interes', 'Retraso_Pago', 'Num_Pagos_Retrasados', 'Cambio_Limite_Credito', \n",
    "    'Num_Consultas_Credito', 'Deuda_Pendiente', 'Ratio_Utilizacion_Credito', \n",
    "    'Edad_Historial_Credito', 'Total_Cuota_Mensual', 'Inversion_Mensual', \n",
    "    'Saldo_Mensual', 'debt_to_income', 'payment_to_income', 'credit_history_ratio', \n",
    "    'delay_ratio', 'credit_usage_to_limit', 'Comportamiento_Bajo_Impacto', \n",
    "    'Comportamiento_Intermedio', 'Comportamiento_Responsable', 'Mezcla_Crediticia_Cod', \n",
    "    'Ocupacion_Architect', 'Ocupacion_Developer', 'Ocupacion_Doctor', \n",
    "    'Ocupacion_Engineer', 'Ocupacion_Entrepreneur', 'Ocupacion_Journalist', \n",
    "    'Ocupacion_Lawyer', 'Ocupacion_Manager', 'Ocupacion_Mechanic', \n",
    "    'Ocupacion_Media_Manager', 'Ocupacion_Musician', 'Ocupacion_Scientist', \n",
    "    'Ocupacion_Teacher', 'Ocupacion_Writer', 'Pago_Minimo_Cod', 'Puntaje_Credito_Num'\n",
    "]\n",
    "\n",
    "# Reorganizar el DataFrame con el orden especificado\n",
    "df = df[columnas_ordenadas]\n",
    "\n",
    "# Verificar las primeras filas del dataset organizado\n",
    "print(\"Primeras filas del dataset organizado:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5086e275",
   "metadata": {},
   "source": [
    "### üìä Mis Observaciones tras Organizar el Dataset\n",
    "\n",
    "¬°El dataset est√° perfectamente organizado! Reorden√© las columnas como quer√≠a, dejando `Puntaje_Credito_Num` al final, y las primeras filas muestran que todo est√° en su lugar: desde `Edad` hasta las codificaciones de `Comportamiento`, `Ocupacion` y `Pago_Minimo_Cod`. Los valores escalados (como `Salario_Mensual` y `Deuda_Pendiente`) y las nuevas caracter√≠sticas est√°n intactos. Con 40 columnas bien estructuradas, mi dataset est√° listo para el modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0b5f9c",
   "metadata": {},
   "source": [
    "## 7. üèÜ Mi Validaci√≥n con RandomForestClassifier\n",
    "En este paso, voy a reentrenar un `RandomForestClassifier` con mis nuevas caracter√≠sticas y las columnas transformadas, para medir su importancia y confirmar que mi dataset est√° optimizado. Esto me permitir√° evaluar c√≥mo mis ajustes y codificaciones impactan la predicci√≥n de **`Puntaje_Credito_Num`**, asegur√°ndome de que todo est√© listo para un modelo efectivo. ¬°A validar se ha dicho!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cbd1104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "\n",
    "# # Separar caracter√≠sticas (X) y variable objetivo (y)\n",
    "# X = df.drop(columns=['Puntaje_Credito_Num'])\n",
    "# y = df['Puntaje_Credito_Num']\n",
    "\n",
    "# # Dividir en conjunto de entrenamiento y prueba\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Entrenar el RandomForestClassifier\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Calcular la importancia de las caracter√≠sticas\n",
    "# importancias = pd.DataFrame({\n",
    "#     'Caracteristica': X.columns,\n",
    "#     'Importancia': rf_model.feature_importances_\n",
    "# }).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "# # Evaluar el modelo en el conjunto de prueba\n",
    "# score = rf_model.score(X_test, y_test)\n",
    "\n",
    "# # Mostrar resultados\n",
    "# print(\"Precisi√≥n del modelo en el conjunto de prueba:\", score)\n",
    "# print(\"\\nImportancia de las caracter√≠sticas:\")\n",
    "# print(importancias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98efaf2",
   "metadata": {},
   "source": [
    "### üìä Mis Observaciones tras Validar con RandomForestClassifier\n",
    "\n",
    "¬°Los resultados son geniales! Entren√© mi `RandomForestClassifier` y obtuve una precisi√≥n del 82.47% en el conjunto de prueba, lo que indica que mi dataset optimizado est√° funcionando bien para predecir **`Puntaje_Credito_Num`**. Al analizar las importancias, veo que `Deuda_Pendiente` (0.09), `Tasa_Interes` (0.07), y `Mezcla_Crediticia_Cod` (0.06) lideran, mostrando su fuerte influencia. Mis nuevas caracter√≠sticas como `credit_history_ratio` (0.05) y `credit_usage_to_limit` (0.046) tambi√©n aportan bastante, validando su creaci√≥n. Las ocupaciones tienen menos peso (todas ~0.002), pero `Pago_Minimo_Cod` (0.016) suma valor. Esto confirma que mis transformaciones y codificaciones dejaron el dataset en gran forma. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e15432",
   "metadata": {},
   "source": [
    "### üèÜ Mi Validaci√≥n con RandomForestClassifier y Muestreo Estratificado\n",
    "En este paso, voy a ajustar mi `RandomForestClassifier` usando muestreo estratificado para dividir mi dataset. Esto significa que separar√© mis datos en entrenamiento y prueba respetando la proporci√≥n de clases en `Puntaje_Credito_Num`, asegur√°ndome de que ambos conjuntos reflejen la distribuci√≥n original. Lo hago para evitar sesgos si las clases est√°n desbalanceadas, lo que podr√≠a mejorar la precisi√≥n del modelo (antes 82.47%) y darme una mejor idea de c√≥mo mis caracter√≠sticas optimizadas predicen **`Puntaje_Credito_Num`**. ¬°A probar esta mejora!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccedf0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "\n",
    "# # Separar caracter√≠sticas (X) y variable objetivo (y)\n",
    "# X = df.drop(columns=['Puntaje_Credito_Num'])\n",
    "# y = df['Puntaje_Credito_Num']\n",
    "\n",
    "# # Dividir en conjunto de entrenamiento y prueba con muestreo estratificado\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Entrenar el RandomForestClassifier\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Calcular la importancia de las caracter√≠sticas\n",
    "# importancias = pd.DataFrame({\n",
    "#     'Caracteristica': X.columns,\n",
    "#     'Importancia': rf_model.feature_importances_\n",
    "# }).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "# # Evaluar el modelo en el conjunto de prueba\n",
    "# score = rf_model.score(X_test, y_test)\n",
    "\n",
    "# # Mostrar resultados\n",
    "# print(\"Precisi√≥n del modelo en el conjunto de prueba (con estratificaci√≥n):\", score)\n",
    "# print(\"\\nImportancia de las caracter√≠sticas:\")\n",
    "# print(importancias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0042c290",
   "metadata": {},
   "source": [
    "### üìä Mis Observaciones tras Validar con Estratificaci√≥n\n",
    "\n",
    "¬°El muestreo estratificado dio un peque√±o empuj√≥n! La precisi√≥n subi√≥ a 82.47% (desde 82.46%), una mejora m√≠nima pero positiva. Las importancias siguen similares, con `Deuda_Pendiente` (0.09) y `Tasa_Interes` (0.07) liderando, y mis nuevas caracter√≠sticas como `credit_history_ratio` (0.05) aportando bien. La estratificaci√≥n asegur√≥ mejor representatividad, pero el impacto es sutil porque las clases no estaban muy desbalanceadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195caf51",
   "metadata": {},
   "source": [
    "## 8. ‚öôÔ∏è Optimizaci√≥n con Distintos Hiperpar√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eaf3448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "# import pandas as pd\n",
    "\n",
    "# # Separar caracter√≠sticas (X) y variable objetivo (y)\n",
    "# X = df.drop(columns=['Puntaje_Credito_Num'])\n",
    "# y = df['Puntaje_Credito_Num']\n",
    "\n",
    "# # Dividir en conjunto de entrenamiento y prueba con muestreo estratificado\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Definir el modelo base con par√°metros por defecto\n",
    "# rf_model = RandomForestClassifier(\n",
    "#     n_estimators=100,\n",
    "#     max_depth=None,\n",
    "#     min_samples_split=5,\n",
    "#     min_samples_leaf=1,\n",
    "#     class_weight='balanced',\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Espacio de b√∫squeda de hiperpar√°metros\n",
    "# param_dist = {\n",
    "#     'n_estimators': [50, 100, 200, 300],\n",
    "#     'max_depth': [10, 20, 30, None],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'class_weight': ['balanced', 'balanced_subsample', None]\n",
    "# }\n",
    "\n",
    "# # Configurar RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=rf_model,\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=20,\n",
    "#     cv=5,\n",
    "#     scoring='f1_macro',\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# # Entrenar con b√∫squeda de hiperpar√°metros\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Obtener el mejor modelo\n",
    "# best_rf_model = random_search.best_estimator_\n",
    "\n",
    "# # Predecir en el conjunto de prueba\n",
    "# y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# # Calcular m√©tricas\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "# f1_per_class = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "# # Calcular importancia de caracter√≠sticas\n",
    "# importancias = pd.DataFrame({\n",
    "#     'Caracteristica': X.columns,\n",
    "#     'Importancia': best_rf_model.feature_importances_\n",
    "# }).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "# # Mostrar resultados\n",
    "# print(\"Mejores hiperpar√°metros encontrados:\", random_search.best_params_)\n",
    "# print(\"\\nM√©tricas en el conjunto de prueba:\")\n",
    "# print(f\"Precisi√≥n (accuracy): {accuracy}\")\n",
    "# print(f\"F1 Macro: {f1_macro}\")\n",
    "# print(f\"F1 por clase (0, 1, 2): {f1_per_class}\")\n",
    "# print(\"\\nImportancia de las caracter√≠sticas:\")\n",
    "# print(importancias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b860e",
   "metadata": {},
   "source": [
    "### üìä Mis Observaciones tras Optimizar con RandomizedSearchCV\n",
    "\n",
    "Optimic√© mi modelo y obtuve `n_estimators: 300`, `min_samples_split: 2`, `min_samples_leaf: 1`, `max_depth: 30`, y `class_weight: balanced`. La precisi√≥n es 82.64%, casi id√©ntica a mi 82.65% anterior, pero el F1 Macro de 0.816 muestra un buen equilibrio. Mis F1 por clase son 0.777 (clase 0), 0.840 (clase 1), y 0.831 (clase 2), destacando en \"Standard\" y \"Good\", aunque \"Poor\" podr√≠a mejorar. `Tasa_Interes` (0.092), `Mezcla_Crediticia_Cod` (0.089), y `Deuda_Pendiente` (0.087) lideran en importancia, mientras las ocupaciones (~0.002) apenas influyen. M√°s √°rboles y la ponderaci√≥n balanceada funcionaron bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e300538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "# import pandas as pd\n",
    "\n",
    "# # Separar caracter√≠sticas (X) y variable objetivo (y)\n",
    "# X = df.drop(columns=['Puntaje_Credito_Num'])\n",
    "# y = df['Puntaje_Credito_Num']\n",
    "\n",
    "# # Dividir en conjunto de entrenamiento y prueba con muestreo estratificado\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Definir el modelo base con par√°metros por defecto\n",
    "# rf_model = RandomForestClassifier(\n",
    "#     n_estimators=300,\n",
    "#     max_depth=30,\n",
    "#     min_samples_split=5,\n",
    "#     min_samples_leaf=1,\n",
    "#     class_weight='balanced',\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Espacio de b√∫squeda de hiperpar√°metros\n",
    "# param_dist = {\n",
    "#     'n_estimators': [200, 400, 800],\n",
    "#     'max_depth': [20, 40, 60, None],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 3],\n",
    "#     'class_weight': ['balanced', None]\n",
    "# }\n",
    "\n",
    "# # Configurar RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=rf_model,\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=15,\n",
    "#     cv=5,\n",
    "#     scoring='f1_macro',\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# # Entrenar con b√∫squeda de hiperpar√°metros\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Obtener el mejor modelo\n",
    "# best_rf_model = random_search.best_estimator_\n",
    "\n",
    "# # Predecir en el conjunto de prueba\n",
    "# y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# # Calcular m√©tricas\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "# f1_per_class = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "# # Calcular importancia de caracter√≠sticas\n",
    "# importancias = pd.DataFrame({\n",
    "#     'Caracteristica': X.columns,\n",
    "#     'Importancia': best_rf_model.feature_importances_\n",
    "# }).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "# # Mostrar resultados\n",
    "# print(\"Mejores hiperpar√°metros encontrados:\", random_search.best_params_)\n",
    "# print(\"\\nM√©tricas en el conjunto de prueba:\")\n",
    "# print(f\"Precisi√≥n (accuracy): {accuracy}\")\n",
    "# print(f\"F1 Macro: {f1_macro}\")\n",
    "# print(f\"F1 por clase (0, 1, 2): {f1_per_class}\")\n",
    "# print(\"\\nImportancia de las caracter√≠sticas:\")\n",
    "# print(importancias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f3b738",
   "metadata": {},
   "source": [
    "### üìä Mis Observaciones tras Optimizar con RandomizedSearchCV\n",
    "\n",
    "Optimic√© mi modelo y consegu√≠ `n_estimators: 400`, `min_samples_split: 5`, `min_samples_leaf: 1`, `max_depth: 40`, y `class_weight: None`. La precisi√≥n subi√≥ a 82.72%, un peque√±o salto desde 82.64%, y mi F1 Macro es 0.817, bastante s√≥lido. Los F1 por clase son 0.778 (clase 0), 0.840 (clase 1), y 0.832 (clase 2), con \"Standard\" liderando. `Deuda_Pendiente` (0.099), `Tasa_Interes` (0.085), y `Mezcla_Crediticia_Cod` (0.069) son mis pilares, mientras las ocupaciones (~0.002) siguen aportando poco. M√°s √°rboles y una profundidad mayor funcionaron bien. ¬øQu√© te parece?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4020eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "# import pandas as pd\n",
    "\n",
    "# # Separar caracter√≠sticas (X) y variable objetivo (y)\n",
    "# X = df.drop(columns=['Puntaje_Credito_Num'])\n",
    "# y = df['Puntaje_Credito_Num']\n",
    "\n",
    "# # Dividir en conjunto de entrenamiento y prueba con muestreo estratificado\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Definir el modelo base con par√°metros por defecto\n",
    "# rf_model = RandomForestClassifier(\n",
    "#     n_estimators=500,\n",
    "#     max_depth=None,\n",
    "#     min_samples_split=2,\n",
    "#     min_samples_leaf=1,\n",
    "#     class_weight='balanced_subsample',\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Espacio de b√∫squeda de hiperpar√°metros\n",
    "# param_dist = {\n",
    "#     'n_estimators': [400, 600, 1000],\n",
    "#     'max_depth': [30, 50, None],\n",
    "#     'min_samples_split': [2, 5],\n",
    "#     'min_samples_leaf': [1, 2],\n",
    "#     'class_weight': ['balanced_subsample', 'balanced']\n",
    "# }\n",
    "\n",
    "# # Configurar RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=rf_model,\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=10,\n",
    "#     cv=5,\n",
    "#     scoring='f1_macro',\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# # Entrenar con b√∫squeda de hiperpar√°metros\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Obtener el mejor modelo\n",
    "# best_rf_model = random_search.best_estimator_\n",
    "\n",
    "# # Predecir en el conjunto de prueba\n",
    "# y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# # Calcular m√©tricas\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "# f1_per_class = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "# # Calcular importancia de caracter√≠sticas\n",
    "# importancias = pd.DataFrame({\n",
    "#     'Caracteristica': X.columns,\n",
    "#     'Importancia': best_rf_model.feature_importances_\n",
    "# }).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "# # Mostrar resultados\n",
    "# print(\"Mejores hiperpar√°metros encontrados:\", random_search.best_params_)\n",
    "# print(\"\\nM√©tricas en el conjunto de prueba:\")\n",
    "# print(f\"Precisi√≥n (accuracy): {accuracy}\")\n",
    "# print(f\"F1 Macro: {f1_macro}\")\n",
    "# print(f\"F1 por clase (0, 1, 2): {f1_per_class}\")\n",
    "# print(\"\\nImportancia de las caracter√≠sticas:\")\n",
    "# print(importancias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0f8e5",
   "metadata": {},
   "source": [
    "### üìä Mis Observaciones tras Optimizar con RandomizedSearchCV\n",
    "\n",
    "Optimic√© mi modelo y obtuve `n_estimators: 600`, `min_samples_split: 5`, `min_samples_leaf: 1`, `max_depth: 30`, y `class_weight: balanced_subsample`. La precisi√≥n subi√≥ a 83.01%, un buen salto desde 82.72%, y mi F1 Macro alcanz√≥ 0.823, mostrando un gran equilibrio. Los F1 por clase son 0.793 (clase 0), 0.844 (clase 1), y 0.831 (clase 2), con \"Standard\" destacando y \"Poor\" mejorando. `Mezcla_Crediticia_Cod` (0.100), `Deuda_Pendiente` (0.096), y `Tasa_Interes` (0.094) son mis pilares, mientras las ocupaciones (~0.002) siguen rezagadas. M√°s √°rboles y la ponderaci√≥n por submuestra dieron un empuj√≥n. ¬°Me encanta este avance! ¬øQu√© opinas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40a65913",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     33\u001b[39m random_search = RandomizedSearchCV(\n\u001b[32m     34\u001b[39m     estimator=rf_model,\n\u001b[32m     35\u001b[39m     param_distributions=param_dist,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m     41\u001b[39m )\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Entrenar con b√∫squeda de hiperpar√°metros\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mrandom_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Obtener el mejor modelo\u001b[39;00m\n\u001b[32m     47\u001b[39m best_rf_model = random_search.best_estimator_\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1951\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1950\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1953\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1954\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1955\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1760\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1761\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1763\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Separar caracter√≠sticas (X) y variable objetivo (y)\n",
    "X = df.drop(columns=['Puntaje_Credito_Num'])\n",
    "y = df['Puntaje_Credito_Num']\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y prueba con muestreo estratificado\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Definir el modelo base con par√°metros de la Configuraci√≥n 3\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=30,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=1,\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Espacio de b√∫squeda combinado de las mejores configuraciones\n",
    "param_dist = {\n",
    "    'n_estimators': [300, 400, 600, 800],  # Incluyo valores altos como 600 y 400\n",
    "    'max_depth': [30, 40, None],           # 30 y 40 destacaron, pruebo sin l√≠mite tambi√©n\n",
    "    'min_samples_split': [2, 5],           # 2 y 5 fueron efectivos\n",
    "    'min_samples_leaf': [1],               # 1 siempre fue √≥ptimo\n",
    "    'class_weight': ['balanced', 'balanced_subsample', None]  # Las tres opciones probadas\n",
    "}\n",
    "\n",
    "# Configurar RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=15,  # M√°s iteraciones para explorar bien\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Entrenar con b√∫squeda de hiperpar√°metros\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_rf_model = random_search.best_estimator_\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# Calcular m√©tricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1_per_class = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "# Calcular importancia de caracter√≠sticas\n",
    "importancias = pd.DataFrame({\n",
    "    'Caracteristica': X.columns,\n",
    "    'Importancia': best_rf_model.feature_importances_\n",
    "}).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Mejores hiperpar√°metros encontrados:\", random_search.best_params_)\n",
    "print(\"\\nM√©tricas en el conjunto de prueba:\")\n",
    "print(f\"Precisi√≥n (accuracy): {accuracy}\")\n",
    "print(f\"F1 Macro: {f1_macro}\")\n",
    "print(f\"F1 por clase (0, 1, 2): {f1_per_class}\")\n",
    "print(\"\\nImportancia de las caracter√≠sticas:\")\n",
    "print(importancias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f95e1",
   "metadata": {},
   "source": [
    "### üìä Mis Observaciones tras Optimizar con RandomizedSearchCV\n",
    "\n",
    "Combin√© lo mejor de mis tres configuraciones anteriores y obtuve `n_estimators: 800`, `min_samples_split: 5`, `min_samples_leaf: 1`, `max_depth: 30`, y `class_weight: balanced_subsample`. La precisi√≥n alcanz√≥ 83.10%, superando mi anterior 83.01%, y mi F1 Macro subi√≥ a 0.824, el mejor hasta ahora. Los F1 por clase son 0.795 (clase 0), 0.844 (clase 1), y 0.832 (clase 2), con \"Poor\" mejorando y \"Standard\" brillando. `Mezcla_Crediticia_Cod` (0.104), `Deuda_Pendiente` (0.095), y `Tasa_Interes` (0.095) lideran, mientras las ocupaciones (~0.002) siguen atr√°s. M√°s √°rboles (800) y la ponderaci√≥n por submuestra fueron clave. ¬°Creo que este es mi mejor modelo hasta ahora!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fe6314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperpar√°metros encontrados: {'n_estimators': 800, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_depth': 30, 'class_weight': 'balanced_subsample'}\n",
      "\n",
      "M√©tricas en el conjunto de prueba:\n",
      "Precisi√≥n (accuracy): 0.8307504788438098\n",
      "F1 Macro: 0.821764885141258\n",
      "F1 por clase (0, 1, 2): [0.78774885 0.84379634 0.83374946]\n",
      "\n",
      "Importancia de las caracter√≠sticas:\n",
      "                 Caracteristica  Importancia\n",
      "23        Mezcla_Crediticia_Cod     0.099920\n",
      "9               Deuda_Pendiente     0.092952\n",
      "4                  Tasa_Interes     0.090899\n",
      "11       Edad_Historial_Credito     0.059346\n",
      "5                  Retraso_Pago     0.043549\n",
      "8         Num_Consultas_Credito     0.041574\n",
      "15               debt_to_income     0.041380\n",
      "17         credit_history_ratio     0.041157\n",
      "19        credit_usage_to_limit     0.039286\n",
      "7         Cambio_Limite_Credito     0.039059\n",
      "16            payment_to_income     0.036796\n",
      "14                Saldo_Mensual     0.033425\n",
      "38              Pago_Minimo_Cod     0.032110\n",
      "18                  delay_ratio     0.031091\n",
      "10    Ratio_Utilizacion_Credito     0.030452\n",
      "12          Total_Cuota_Mensual     0.029475\n",
      "1               Salario_Mensual     0.028881\n",
      "0                          Edad     0.028735\n",
      "6          Num_Pagos_Retrasados     0.028595\n",
      "3          Num_Tarjetas_Credito     0.027892\n",
      "13            Inversion_Mensual     0.027663\n",
      "2         Num_Cuentas_Bancarias     0.026435\n",
      "20  Comportamiento_Bajo_Impacto     0.006242\n",
      "21    Comportamiento_Intermedio     0.005409\n",
      "22   Comportamiento_Responsable     0.002948\n",
      "25          Ocupacion_Developer     0.002846\n",
      "26             Ocupacion_Doctor     0.002699\n",
      "35          Ocupacion_Scientist     0.002631\n",
      "30             Ocupacion_Lawyer     0.002532\n",
      "33      Ocupacion_Media_Manager     0.002512\n",
      "28       Ocupacion_Entrepreneur     0.002481\n",
      "36            Ocupacion_Teacher     0.002478\n",
      "32           Ocupacion_Mechanic     0.002454\n",
      "24          Ocupacion_Architect     0.002448\n",
      "37             Ocupacion_Writer     0.002406\n",
      "34           Ocupacion_Musician     0.002395\n",
      "31            Ocupacion_Manager     0.002345\n",
      "27           Ocupacion_Engineer     0.002324\n",
      "29         Ocupacion_Journalist     0.002176\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Separar caracter√≠sticas (X) y variable objetivo (y)\n",
    "X = df.drop(columns=['Puntaje_Credito_Num'])\n",
    "y = df['Puntaje_Credito_Num']\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y prueba con muestreo estratificado\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Definir el modelo base\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Espacio de b√∫squeda de hiperpar√°metros\n",
    "param_dist = {\n",
    "    'n_estimators': [800, 1000, 1200],    # M√°s √°rboles para mayor capacidad\n",
    "    'max_depth': [30, 50, 70, None],      # Profundidades mayores\n",
    "    'min_samples_split': [2, 3],          # M√°s flexibilidad en divisiones\n",
    "    'min_samples_leaf': [1],              # Mantenemos 1, fue consistente\n",
    "    'class_weight': ['balanced_subsample', 'balanced']  # Foco en balance\n",
    "}\n",
    "\n",
    "# Configurar RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=15,  # M√°s iteraciones para explorar\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Entrenar con b√∫squeda de hiperpar√°metros\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_rf_model = random_search.best_estimator_\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# Calcular m√©tricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1_per_class = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "# Calcular importancia de caracter√≠sticas\n",
    "importancias = pd.DataFrame({\n",
    "    'Caracteristica': X.columns,\n",
    "    'Importancia': best_rf_model.feature_importances_\n",
    "}).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Mejores hiperpar√°metros encontrados:\", random_search.best_params_)\n",
    "print(\"\\nM√©tricas en el conjunto de prueba:\")\n",
    "print(f\"Precisi√≥n (accuracy): {accuracy}\")\n",
    "print(f\"F1 Macro: {f1_macro}\")\n",
    "print(f\"F1 por clase (0, 1, 2): {f1_per_class}\")\n",
    "print(\"\\nImportancia de las caracter√≠sticas:\")\n",
    "print(importancias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b62c11",
   "metadata": {},
   "source": [
    "### üöÄ Mi Prueba con XGBoost\n",
    "¬°Es hora de subir el nivel! Voy a probar un `XGBoost` para ver si supero mi mejor precisi√≥n (83.1%) y me acerco al 89-90%. Usar√© `RandomizedSearchCV` para optimizar hiperpar√°metros como n√∫mero de √°rboles, profundidad y tasa de aprendizaje, enfoc√°ndome en `f1_macro` para balancear mis clases. Con este modelo m√°s potente, espero sacarle el m√°ximo jugo a mis datos y predecir **`Puntaje_Credito_Num`** con mayor precisi√≥n. ¬°A por ello!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb53a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [19:50:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperpar√°metros encontrados: {'subsample': 0.9, 'n_estimators': 400, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.9}\n",
      "\n",
      "M√©tricas en el conjunto de prueba:\n",
      "Precisi√≥n (accuracy): 0.8196064774508097\n",
      "F1 Macro: 0.8076440493046183\n",
      "F1 por clase (0, 1, 2): [0.76446669 0.83289391 0.82557155]\n",
      "\n",
      "Importancia de las caracter√≠sticas:\n",
      "                 Caracteristica  Importancia\n",
      "23        Mezcla_Crediticia_Cod     0.485894\n",
      "9               Deuda_Pendiente     0.061589\n",
      "4                  Tasa_Interes     0.030440\n",
      "3          Num_Tarjetas_Credito     0.020631\n",
      "16            payment_to_income     0.015338\n",
      "7         Cambio_Limite_Credito     0.014015\n",
      "26             Ocupacion_Doctor     0.013712\n",
      "25          Ocupacion_Developer     0.013510\n",
      "8         Num_Consultas_Credito     0.013176\n",
      "5                  Retraso_Pago     0.013072\n",
      "1               Salario_Mensual     0.013067\n",
      "30             Ocupacion_Lawyer     0.013066\n",
      "37             Ocupacion_Writer     0.012958\n",
      "35          Ocupacion_Scientist     0.012898\n",
      "34           Ocupacion_Musician     0.012736\n",
      "33      Ocupacion_Media_Manager     0.012662\n",
      "13            Inversion_Mensual     0.012646\n",
      "28       Ocupacion_Entrepreneur     0.012600\n",
      "12          Total_Cuota_Mensual     0.012561\n",
      "27           Ocupacion_Engineer     0.012285\n",
      "15               debt_to_income     0.012271\n",
      "2         Num_Cuentas_Bancarias     0.012219\n",
      "31            Ocupacion_Manager     0.011987\n",
      "29         Ocupacion_Journalist     0.011546\n",
      "36            Ocupacion_Teacher     0.011423\n",
      "24          Ocupacion_Architect     0.011363\n",
      "19        credit_usage_to_limit     0.011239\n",
      "32           Ocupacion_Mechanic     0.011113\n",
      "20  Comportamiento_Bajo_Impacto     0.010719\n",
      "0                          Edad     0.010480\n",
      "18                  delay_ratio     0.009964\n",
      "38              Pago_Minimo_Cod     0.009963\n",
      "11       Edad_Historial_Credito     0.009747\n",
      "6          Num_Pagos_Retrasados     0.009638\n",
      "17         credit_history_ratio     0.009312\n",
      "21    Comportamiento_Intermedio     0.007946\n",
      "14                Saldo_Mensual     0.007125\n",
      "10    Ratio_Utilizacion_Credito     0.006625\n",
      "22   Comportamiento_Responsable     0.006463\n"
     ]
    }
   ],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "# import pandas as pd\n",
    "\n",
    "# # Separar caracter√≠sticas (X) y variable objetivo (y)\n",
    "# X = df.drop(columns=['Puntaje_Credito_Num'])\n",
    "# y = df['Puntaje_Credito_Num']\n",
    "\n",
    "# # Dividir en conjunto de entrenamiento y prueba con muestreo estratificado\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Definir el modelo base de XGBoost\n",
    "# xgb_model = XGBClassifier(\n",
    "#     n_estimators=500,\n",
    "#     max_depth=6,\n",
    "#     learning_rate=0.1,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     random_state=42,\n",
    "#     eval_metric='mlogloss',  # Para multiclase\n",
    "#     use_label_encoder=False\n",
    "# )\n",
    "\n",
    "# # Espacio de b√∫squeda de hiperpar√°metros\n",
    "# param_dist = {\n",
    "#     'n_estimators': [400, 600, 800],      # N√∫mero de √°rboles\n",
    "#     'max_depth': [4, 6, 8],               # Profundidad m√°xima\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],    # Tasa de aprendizaje\n",
    "#     'subsample': [0.7, 0.8, 0.9],         # Fracci√≥n de muestras\n",
    "#     'colsample_bytree': [0.7, 0.8, 0.9]   # Fracci√≥n de caracter√≠sticas\n",
    "# }\n",
    "\n",
    "# # Configurar RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=xgb_model,\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=15,\n",
    "#     cv=5,\n",
    "#     scoring='f1_macro',\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# # Entrenar con b√∫squeda de hiperpar√°metros\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Obtener el mejor modelo\n",
    "# best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# # Predecir en el conjunto de prueba\n",
    "# y_pred = best_xgb_model.predict(X_test)\n",
    "\n",
    "# # Calcular m√©tricas\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "# f1_per_class = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "# # Calcular importancia de caracter√≠sticas\n",
    "# importancias = pd.DataFrame({\n",
    "#     'Caracteristica': X.columns,\n",
    "#     'Importancia': best_xgb_model.feature_importances_\n",
    "# }).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "# # Mostrar resultados\n",
    "# print(\"Mejores hiperpar√°metros encontrados:\", random_search.best_params_)\n",
    "# print(\"\\nM√©tricas en el conjunto de prueba:\")\n",
    "# print(f\"Precisi√≥n (accuracy): {accuracy}\")\n",
    "# print(f\"F1 Macro: {f1_macro}\")\n",
    "# print(f\"F1 por clase (0, 1, 2): {f1_per_class}\")\n",
    "# print(\"\\nImportancia de las caracter√≠sticas:\")\n",
    "# print(importancias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef63a9",
   "metadata": {},
   "source": [
    "### üìä Mis Observaciones tras Probar XGBoost\n",
    "\n",
    "¬°Bueno, parece que RandomForest sigue siendo el rey por ahora! Con XGBoost obtuve `n_estimators: 400`, `max_depth: 8`, `learning_rate: 0.1`, `subsample: 0.9`, y `colsample_bytree: 0.9`. La precisi√≥n cay√≥ a 81.96% (lejos de mi 83.1%), y el F1 Macro qued√≥ en 0.808, con F1 por clase en 0.764 (clase 0), 0.833 (clase 1), y 0.826 (clase 2). \"Poor\" baj√≥ bastante. `Mezcla_Crediticia_Cod` (0.486) domina las importancias, pero el resto se reparte poco."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb9035",
   "metadata": {},
   "source": [
    "### üìä Volvemos a RandomForest pero esta vez aumentare la complejidad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc45970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "30 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"g:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"g:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\sklearn\\base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"g:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\sklearn\\base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"g:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"g:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"g:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\sklearn\\base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"g:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\sklearn\\base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"g:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "g:\\MLOps Proyecto End_to_End\\env_pipeline\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.80690379        nan 0.80521579        nan        nan        nan\n",
      "        nan        nan 0.80687687 0.80543737 0.8124108  0.80592994\n",
      " 0.80549397 0.8116299  0.80797069 0.80743081 0.8122488  0.80600347\n",
      " 0.80531183 0.80531373]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperpar√°metros encontrados: {'n_estimators': 1000, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 40, 'class_weight': 'balanced'}\n",
      "\n",
      "M√©tricas en el conjunto de prueba:\n",
      "Precisi√≥n (accuracy): 0.8307504788438098\n",
      "F1 Macro: 0.8228081587330563\n",
      "F1 por clase (0, 1, 2): [0.79149191 0.84452645 0.83240612]\n",
      "\n",
      "Importancia de las caracter√≠sticas:\n",
      "                 Caracteristica  Importancia\n",
      "23        Mezcla_Crediticia_Cod     0.106942\n",
      "9               Deuda_Pendiente     0.094223\n",
      "4                  Tasa_Interes     0.091498\n",
      "11       Edad_Historial_Credito     0.055887\n",
      "8         Num_Consultas_Credito     0.045732\n",
      "15               debt_to_income     0.043466\n",
      "5                  Retraso_Pago     0.043226\n",
      "19        credit_usage_to_limit     0.039927\n",
      "7         Cambio_Limite_Credito     0.039022\n",
      "16            payment_to_income     0.037804\n",
      "17         credit_history_ratio     0.037694\n",
      "38              Pago_Minimo_Cod     0.032675\n",
      "12          Total_Cuota_Mensual     0.030259\n",
      "14                Saldo_Mensual     0.029823\n",
      "1               Salario_Mensual     0.029375\n",
      "18                  delay_ratio     0.029291\n",
      "0                          Edad     0.028540\n",
      "13            Inversion_Mensual     0.028158\n",
      "6          Num_Pagos_Retrasados     0.027876\n",
      "3          Num_Tarjetas_Credito     0.027574\n",
      "2         Num_Cuentas_Bancarias     0.027275\n",
      "10    Ratio_Utilizacion_Credito     0.026681\n",
      "20  Comportamiento_Bajo_Impacto     0.005060\n",
      "21    Comportamiento_Intermedio     0.004333\n",
      "25          Ocupacion_Developer     0.002936\n",
      "26             Ocupacion_Doctor     0.002748\n",
      "35          Ocupacion_Scientist     0.002685\n",
      "30             Ocupacion_Lawyer     0.002589\n",
      "33      Ocupacion_Media_Manager     0.002548\n",
      "36            Ocupacion_Teacher     0.002526\n",
      "28       Ocupacion_Entrepreneur     0.002518\n",
      "24          Ocupacion_Architect     0.002488\n",
      "32           Ocupacion_Mechanic     0.002473\n",
      "34           Ocupacion_Musician     0.002442\n",
      "22   Comportamiento_Responsable     0.002419\n",
      "37             Ocupacion_Writer     0.002412\n",
      "31            Ocupacion_Manager     0.002375\n",
      "27           Ocupacion_Engineer     0.002321\n",
      "29         Ocupacion_Journalist     0.002179\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "# import pandas as pd\n",
    "\n",
    "# # Separar caracter√≠sticas (X) y variable objetivo (y)\n",
    "# X = df.drop(columns=['Puntaje_Credito_Num'])\n",
    "# y = df['Puntaje_Credito_Num']\n",
    "\n",
    "# # Dividir en conjunto de entrenamiento y prueba con muestreo estratificado\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# # Definir el modelo base (partimos del mejor anterior)\n",
    "# rf_model = RandomForestClassifier(\n",
    "#     n_estimators=800,\n",
    "#     max_depth=30,\n",
    "#     min_samples_split=5,\n",
    "#     min_samples_leaf=1,\n",
    "#     class_weight='balanced_subsample',\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Espacio de b√∫squeda de hiperpar√°metros ampliado\n",
    "# param_dist = {\n",
    "#     'n_estimators': [800, 1000, 1200, 1500],    # M√°s √°rboles para mayor potencia\n",
    "#     'max_depth': [30, 40, 50, None],            # Profundidades mayores\n",
    "#     'min_samples_split': [2, 3, 5],             # Flexibilidad en divisiones\n",
    "#     'min_samples_leaf': [1, 2],                 # Regularizaci√≥n ligera\n",
    "#     'max_features': ['auto', 'sqrt', 0.7],      # Diversidad en caracter√≠sticas\n",
    "#     'class_weight': ['balanced_subsample', 'balanced', None]  # Opciones de balance\n",
    "# }\n",
    "\n",
    "# # Configurar RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=rf_model,\n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=20,  # M√°s iteraciones para explorar\n",
    "#     cv=5,\n",
    "#     scoring='f1_macro',\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# # Entrenar con b√∫squeda de hiperpar√°metros\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Obtener el mejor modelo\n",
    "# best_rf_model = random_search.best_estimator_\n",
    "\n",
    "# # Predecir en el conjunto de prueba\n",
    "# y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# # Calcular m√©tricas\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "# f1_per_class = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "# # Calcular importancia de caracter√≠sticas\n",
    "# importancias = pd.DataFrame({\n",
    "#     'Caracteristica': X.columns,\n",
    "#     'Importancia': best_rf_model.feature_importances_\n",
    "# }).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "# # Mostrar resultados\n",
    "# print(\"Mejores hiperpar√°metros encontrados:\", random_search.best_params_)\n",
    "# print(\"\\nM√©tricas en el conjunto de prueba:\")\n",
    "# print(f\"Precisi√≥n (accuracy): {accuracy}\")\n",
    "# print(f\"F1 Macro: {f1_macro}\")\n",
    "# print(f\"F1 por clase (0, 1, 2): {f1_per_class}\")\n",
    "# print(\"\\nImportancia de las caracter√≠sticas:\")\n",
    "# print(importancias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd69ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo 'clean_credit_clients.csv' ha sido guardado exitosamente en la carpeta 'data/processed/clean_data'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Crear la carpeta 'clean_data' si no existe\n",
    "if not os.path.exists('../data/processed/clean_data'):\n",
    "    os.makedirs('clean_data')\n",
    "\n",
    "# Guardar el DataFrame en un archivo CSV dentro de la carpeta 'clean_data'\n",
    "df.to_csv('../data/processed/engineering_processed/clean_credit_clients.csv', index=False)\n",
    "\n",
    "print(\"El archivo 'clean_credit_clients.csv' ha sido guardado exitosamente en la carpeta 'data/processed/clean_data'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b39737b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√©tricas en el conjunto de prueba:\n",
      "Precisi√≥n (accuracy): 0.8300539787567474\n",
      "F1 Macro: 0.8226675161697118\n",
      "F1 por clase (0, 1, 2): [0.79310345 0.84365782 0.83124128]\n",
      "Precision Macro: 0.8150332123456163\n",
      "Recall Macro: 0.8337600218360387\n",
      "Balanced Accuracy: 0.8337600218360387\n",
      "ROC AUC (ovr): 0.9311559889731998\n",
      "Matriz de Confusi√≥n:\n",
      "[[1334    7  300]\n",
      " [   9 3432  393]\n",
      " [ 380  863 4768]]\n",
      "\n",
      "F1 Macro (validaci√≥n cruzada, 5 folds): 0.6826683369674897\n",
      "\n",
      "Importancia de las caracter√≠sticas:\n",
      "                 Caracteristica  Importancia\n",
      "23        Mezcla_Crediticia_Cod     0.099795\n",
      "9               Deuda_Pendiente     0.096246\n",
      "4                  Tasa_Interes     0.094280\n",
      "11       Edad_Historial_Credito     0.057951\n",
      "5                  Retraso_Pago     0.044580\n",
      "8         Num_Consultas_Credito     0.041888\n",
      "15               debt_to_income     0.041802\n",
      "19        credit_usage_to_limit     0.041201\n",
      "7         Cambio_Limite_Credito     0.039241\n",
      "16            payment_to_income     0.037876\n",
      "17         credit_history_ratio     0.037520\n",
      "38              Pago_Minimo_Cod     0.034353\n",
      "12          Total_Cuota_Mensual     0.030053\n",
      "1               Salario_Mensual     0.029459\n",
      "18                  delay_ratio     0.029390\n",
      "3          Num_Tarjetas_Credito     0.029372\n",
      "14                Saldo_Mensual     0.029294\n",
      "6          Num_Pagos_Retrasados     0.029030\n",
      "0                          Edad     0.028534\n",
      "13            Inversion_Mensual     0.028061\n",
      "2         Num_Cuentas_Bancarias     0.027256\n",
      "10    Ratio_Utilizacion_Credito     0.026089\n",
      "20  Comportamiento_Bajo_Impacto     0.005052\n",
      "21    Comportamiento_Intermedio     0.004238\n",
      "25          Ocupacion_Developer     0.002918\n",
      "26             Ocupacion_Doctor     0.002770\n",
      "35          Ocupacion_Scientist     0.002672\n",
      "30             Ocupacion_Lawyer     0.002593\n",
      "28       Ocupacion_Entrepreneur     0.002541\n",
      "33      Ocupacion_Media_Manager     0.002532\n",
      "36            Ocupacion_Teacher     0.002499\n",
      "34           Ocupacion_Musician     0.002449\n",
      "24          Ocupacion_Architect     0.002433\n",
      "37             Ocupacion_Writer     0.002423\n",
      "32           Ocupacion_Mechanic     0.002416\n",
      "22   Comportamiento_Responsable     0.002393\n",
      "31            Ocupacion_Manager     0.002371\n",
      "27           Ocupacion_Engineer     0.002284\n",
      "29         Ocupacion_Journalist     0.002146\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, \n",
    "                             balanced_accuracy_score, roc_auc_score, confusion_matrix)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Separar caracter√≠sticas (X) y variable objetivo (y)\n",
    "X = df.drop(columns=['Puntaje_Credito_Num'])\n",
    "y = df['Puntaje_Credito_Num']\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y prueba con muestreo estratificado\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Definir el modelo con los par√°metros espec√≠ficos\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=30,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=1,\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calcular m√©tricas en el conjunto de prueba\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1_per_class = f1_score(y_test, y_pred, average=None)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "# ROC AUC (one-vs-rest) requiere probabilidades\n",
    "y_pred_proba = rf_model.predict_proba(X_test)\n",
    "roc_auc_ovr = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Validaci√≥n cruzada para f1_macro (scoring principal)\n",
    "cv_f1_macro = cross_val_score(rf_model, X, y, cv=5, scoring='f1_macro').mean()\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"M√©tricas en el conjunto de prueba:\")\n",
    "print(f\"Precisi√≥n (accuracy): {accuracy}\")\n",
    "print(f\"F1 Macro: {f1_macro}\")\n",
    "print(f\"F1 por clase (0, 1, 2): {f1_per_class}\")\n",
    "print(f\"Precision Macro: {precision_macro}\")\n",
    "print(f\"Recall Macro: {recall_macro}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy}\")\n",
    "print(f\"ROC AUC (ovr): {roc_auc_ovr}\")\n",
    "print(f\"Matriz de Confusi√≥n:\\n{conf_matrix}\")\n",
    "print(f\"\\nF1 Macro (validaci√≥n cruzada, 5 folds): {cv_f1_macro}\")\n",
    "\n",
    "# Importancia de las caracter√≠sticas\n",
    "importancias = pd.DataFrame({\n",
    "    'Caracteristica': X.columns,\n",
    "    'Importancia': rf_model.feature_importances_\n",
    "}).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "print(\"\\nImportancia de las caracter√≠sticas:\")\n",
    "print(importancias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8836db15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√©tricas en el conjunto de prueba:\n",
      "Precisi√≥n (accuracy): 0.8310116663764583\n",
      "F1 Macro: 0.8239003727097365\n",
      "F1 por clase (0, 1, 2): [0.79547484 0.84408404 0.83214223]\n",
      "Precision Macro: 0.8164788857437206\n",
      "Recall Macro: 0.8347598257288227\n",
      "Balanced Accuracy: 0.8347598257288227\n",
      "ROC AUC (ovr): 0.9311895882058391\n",
      "Matriz de Confusi√≥n:\n",
      "[[1336    7  298]\n",
      " [   8 3435  391]\n",
      " [ 374  863 4774]]\n",
      "\n",
      "F1 Macro (validaci√≥n cruzada, 5 folds): 0.6831677519046938\n",
      "\n",
      "Importancia de las caracter√≠sticas:\n",
      "                 Caracteristica  Importancia\n",
      "23        Mezcla_Crediticia_Cod     0.103507\n",
      "9               Deuda_Pendiente     0.095258\n",
      "4                  Tasa_Interes     0.094843\n",
      "11       Edad_Historial_Credito     0.056694\n",
      "5                  Retraso_Pago     0.044166\n",
      "15               debt_to_income     0.042262\n",
      "8         Num_Consultas_Credito     0.042100\n",
      "19        credit_usage_to_limit     0.040911\n",
      "7         Cambio_Limite_Credito     0.039511\n",
      "16            payment_to_income     0.037748\n",
      "17         credit_history_ratio     0.037250\n",
      "38              Pago_Minimo_Cod     0.034098\n",
      "12          Total_Cuota_Mensual     0.030184\n",
      "1               Salario_Mensual     0.029334\n",
      "14                Saldo_Mensual     0.029199\n",
      "18                  delay_ratio     0.029173\n",
      "3          Num_Tarjetas_Credito     0.028928\n",
      "6          Num_Pagos_Retrasados     0.028350\n",
      "0                          Edad     0.028302\n",
      "13            Inversion_Mensual     0.027992\n",
      "2         Num_Cuentas_Bancarias     0.027406\n",
      "10    Ratio_Utilizacion_Credito     0.026039\n",
      "20  Comportamiento_Bajo_Impacto     0.005037\n",
      "21    Comportamiento_Intermedio     0.004254\n",
      "25          Ocupacion_Developer     0.002909\n",
      "26             Ocupacion_Doctor     0.002781\n",
      "35          Ocupacion_Scientist     0.002671\n",
      "30             Ocupacion_Lawyer     0.002589\n",
      "36            Ocupacion_Teacher     0.002532\n",
      "28       Ocupacion_Entrepreneur     0.002523\n",
      "33      Ocupacion_Media_Manager     0.002512\n",
      "34           Ocupacion_Musician     0.002459\n",
      "24          Ocupacion_Architect     0.002455\n",
      "32           Ocupacion_Mechanic     0.002425\n",
      "37             Ocupacion_Writer     0.002411\n",
      "22   Comportamiento_Responsable     0.002397\n",
      "31            Ocupacion_Manager     0.002364\n",
      "27           Ocupacion_Engineer     0.002285\n",
      "29         Ocupacion_Journalist     0.002142\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, \n",
    "                             balanced_accuracy_score, roc_auc_score, confusion_matrix)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Separar caracter√≠sticas (X) y variable objetivo (y)\n",
    "X = df.drop(columns=['Puntaje_Credito_Num'])\n",
    "y = df['Puntaje_Credito_Num']\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y prueba con muestreo estratificado\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Definir el modelo con los par√°metros espec√≠ficos\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=800,\n",
    "    max_depth=30,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=1,\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calcular m√©tricas en el conjunto de prueba\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1_per_class = f1_score(y_test, y_pred, average=None)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "# ROC AUC (one-vs-rest) requiere probabilidades\n",
    "y_pred_proba = rf_model.predict_proba(X_test)\n",
    "roc_auc_ovr = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Validaci√≥n cruzada para f1_macro (scoring principal)\n",
    "cv_f1_macro = cross_val_score(rf_model, X, y, cv=5, scoring='f1_macro').mean()\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"M√©tricas en el conjunto de prueba:\")\n",
    "print(f\"Precisi√≥n (accuracy): {accuracy}\")\n",
    "print(f\"F1 Macro: {f1_macro}\")\n",
    "print(f\"F1 por clase (0, 1, 2): {f1_per_class}\")\n",
    "print(f\"Precision Macro: {precision_macro}\")\n",
    "print(f\"Recall Macro: {recall_macro}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy}\")\n",
    "print(f\"ROC AUC (ovr): {roc_auc_ovr}\")\n",
    "print(f\"Matriz de Confusi√≥n:\\n{conf_matrix}\")\n",
    "print(f\"\\nF1 Macro (validaci√≥n cruzada, 5 folds): {cv_f1_macro}\")\n",
    "\n",
    "# Importancia de las caracter√≠sticas\n",
    "importancias = pd.DataFrame({\n",
    "    'Caracteristica': X.columns,\n",
    "    'Importancia': rf_model.feature_importances_\n",
    "}).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "print(\"\\nImportancia de las caracter√≠sticas:\")\n",
    "print(importancias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
